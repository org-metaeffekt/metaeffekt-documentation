> [Documentation](../../README.md) >
> [Vulnerability Management](../vulnerability-management.md) >
> [Vulnerability Data Mirror](vulnerability-data-mirror.md) >
> Indexers

# Indexers

> [Overview](#overview) -
> [POM Configuration](#pom-configuration) -
> [Indexing Workflow](#indexing-workflow) -
> [Indexers Details](#indexers-details) -
> [Global Configurations](#global-configurations)

## Overview

The indexers extract and process data from the downloads, organizing it into searchable formats for further use.
Each indexer processes data differently depending on the structure and type of data received from its associated
downloads.
A detailed overview of the dependencies between downloads and indexes is available on the individual index chapters
as seen below.

- [NVD CVE API](#nvd-cve-api)
- [NVD CPE API](#nvd-cpe-api)
- [NVD CPE API Vendor Product](#nvd-cpe-api-vendor-product)
- [MSRC Product](#msrc-product)
- [MSRC Advisor](#msrc-advisor)
- [MSRC KB Chain](#msrc-kb-chain)
- [OSV Advisor](#osv-advisor)
- [CSAF Vulnerability Matching](#csaf-vulnerability-matching)
- [CSAF Advisory](#csaf-advisory)
- [CERT-SEI Advisor](#cert-sei-advisor)
- [CERT-FR Advisor](#cert-fr-advisor)
- [CERT-EU Advisor](#cert-eu-advisor)
- [KEV](#kev)
- [EPSS](#epss)
- [EOL](#eol)
- [CWE](#cwe)
- [CAPEC](#capec)

An overview diagram for all available downloaders, indexers, and enrichers is available in the
[Large Overview Diagram](../diagrams/individual-index-diagrams.md) chapter.

#### Deprecated Downloaders

- [_NVD Vulnerability (deprecated)_](#nvd-vulnerability-deprecated)
- [_CPE Dictionary (deprecated)_](#cpe-dictionary-deprecated)
- [_CPE Dictionary Vendor Product (deprecated)_](#cpe-dictionary-vendor-product-deprecated)

## POM Configuration

The downloader configuration is part of the `ae-mirror-plugin` configuration in the Maven POM.
An example configuration with all available indexers is provided below.

<details>

<summary>Click to show the full POM configuration</summary>

<!-- @formatter:off -->
```xml
<plugins>
    <plugin>
        <groupId>com.metaeffekt.artifact.analysis</groupId>
        <artifactId>ae-mirror-plugin</artifactId>
        <version>${ae.artifact.analysis.version}</version>

        <executions>
            <execution>
                <id>data-mirror-index</id>
                <goals>
                    <goal>data-mirror</goal>
                </goals>

                <configuration>
                    <!-- General Configuration -->
                    <mirrorDirectory>${env.mirror.dir}</mirrorDirectory>

                    <!-- Indexers -->
                    <nvdVulnerabilityIndex/>
                    <nvdCpeIndex/>
                    <nvdCpeVendorProductIndex/>
                    <msrcProductIndex/>
                    <msrcAdvisorIndex/>
                    <msrcKbChainIndex/>
                    
                    <osvAdvisorIndex>
                        <useDefaultExcludeAdvisoryProviders>true</useDefaultExcludeAdvisoryProviders>
                        <excludeAdvisoryProviders>[]</excludeAdvisoryProviders>
                    </osvAdvisorIndex>
                    
                    <csafVulnerabilityIndex/>
                    <csafAdvisoryIndex/>
                    <certSeiAdvisorIndex/>
                    <certFrAdvisorIndex/>
                    <certEuAdvisorIndex/>
                    <kevIndex/>
                    <epssIndex/>
                    <eolIndex/>
                    <cweIndex/>
                    <capecIndex/>
                    
                    <!-- Deprecated steps -->
                    <!-- <nvdLegacyVulnerabilityIndex/> -->
                    <!-- <cpeDictionaryIndex/> -->
                    <!-- <cpeDictionaryVendorProductIndex/> -->

                    <!-- Vendors sometimes publish security information containing invalid CPE Format Strings.
                         The <cpeCorrections> and <cpePatternCorrections> properties allow to correct these malformed CPE Strings -->

                    <!-- 1. replace a plain CPE String (<malformedCpe>) with a replacement CPE String (<replacementCpe>) -->
                    <cpeCorrections>
                        <cpeCorrection> <!-- part and vendor melted together -->
                            <malformedCpe>cpe:2.3:aainwp:onestore_sites:*:*:*:*:*:*:wordpress:*:*</malformedCpe>
                            <replacementCpe>cpe:2.3:a:sainwp:onestore_sites:*:*:*:*:*:wordpress:*:*</replacementCpe>
                        </cpeCorrection>
                    </cpeCorrections>

                    <!-- 2. match a regular expression (<malformedCpe>) against the malformed CPE String to be replaced with an expression (<replacementCpe>) that may reference capturing groups from the pattern
                            the <malformedCpe> must be enclosed with '/' characters -->
                    <cpePatternCorrections>
                        <!-- GENERAL REGEX CORRECTIONS -->

                        <cpeCorrection> <!-- trim whitespaces from beginning and end -->
                            <malformedCpe>/^\s+(.*?)\s+$/</malformedCpe>
                            <replacementCpe>$1</replacementCpe>
                        </cpeCorrection>
                        <cpeCorrection> <!-- whitespaces surrounding the delimiter character ':' are collapsed -->
                            <malformedCpe>/: +| +:/</malformedCpe>
                            <replacementCpe>:</replacementCpe>
                        </cpeCorrection>
                        <cpeCorrection> <!-- any other remaining whitespaces are replaced with the '_' character, as defined by the cpe spec -->
                            <malformedCpe>/ +/</malformedCpe>
                            <replacementCpe>_</replacementCpe>
                        </cpeCorrection>
                        <cpeCorrection> <!-- cpes may not contain multiple asterisk characters in sequence and are therefore collapsed as well -->
                            <malformedCpe>/\*\*/</malformedCpe>
                            <replacementCpe>*</replacementCpe>
                        </cpeCorrection>
                        <cpeCorrection> <!-- another common mistake is doubling the cpe header; if present, it is collapsed and the first configured part is used -->
                            <malformedCpe>/^cpe:2\.3:([aoh]):cpe:2\.3:[aoh]/</malformedCpe>
                            <replacementCpe>cpe:2.3:$1</replacementCpe>
                        </cpeCorrection>

                        <!-- SPECIFIC REGEX CORRECTIONS -->

                        <cpeCorrection> <!-- non-escaped ":" -->
                            <malformedCpe>/cpe:2.3:a:ibm:sterling_connect:(.*?):(.*?):\*:\*:\*:\*:(.*?):\*:\*/</malformedCpe>
                            <replacementCpe>cpe:2.3:a:ibm:sterling_connect\\:$1:$2:*:*:*:*:$3:*:*</replacementCpe>
                        </cpeCorrection>
                    </cpePatternCorrections>
                </configuration>
            </execution>
        </executions>
    </plugin>
</plugins>
```
<!-- @formatter:on -->

</details>

## Indexing Workflow

The indexing process is responsible for transforming downloaded data into Lucene Indexes,
where each index consists of a set of documents structured as key-value pairs.
The specific data captured in these documents varies between indexes, based on the type of data they handle.

Indexers may depend on one or more downloads or other indexes.
If any required data or index is missing, the indexing process will not proceed.

Indexing is initiated under the following conditions:

- The index has not been created before.
- The index has failed in its last run.
- The index is older than a configured threshold.
- Any of the downloads or indexes it depends on have been updated since the last indexing.

<!--### Common Steps in the Indexing Process

While each indexer functions differently depending on the type of data it handles,
the following general steps are followed:

1. **Wait for Directory Unlock**: Before indexing, the directory must be unlocked to ensure other processes are not
   accessing or modifying the data.
2. **Lock Directory**: Once available, the directory is locked to prevent concurrent read/write operations.
3. **Check for Updates**: The system checks whether indexing is necessary based on the defined conditions. If no update
   is required, the process is aborted.
4. **Backup Existing Index**: A backup of the current index is made to prevent data loss in case of failure.
5. **Clear Current Index**: The existing index data is fully removed to prepare for the new data.
6. **Create Index**:
    - Validate that all required downloads and dependencies are available.
    - Generate the index documents, which differ by indexer.
    - Write the documents to the index directory if at least one document has been created.
7. **Handle Exceptions**: If an error occurs, the system reverts to the backup. Regardless of the outcome, the backup is
   removed after the process.
8. **Unlock Directory**: Once the indexing process is complete, the directory is unlocked to allow other processes
   access.-->

It cannot be stressed enough that since each indexer works with different data formats and sources,
they each require very specialized parsing methods.
These sources vary widely and may lack a consistent or machine-readable format, making the indexing process complex.
This is why a clear and defined preprocessing in form of a mapping of source fields to target fields in the Lucene index
is so important to the process.

# Indexers Details

## NVD CVE API

`nvd-cve` / `nvdVulnerabilityIndex`

<details>

<summary>Source Code for <code>NvdCveApiIndex.createIndexDocuments()</code></summary>

```java
final NvdVulnerabilityParser vulnerabilityParser = super.configureDataParser(new NvdVulnerabilityParser());
final CveListData cveListData = prepareCveListData();

this.processDocuments(vulnerabilityParser, cveListData);

return Collections.emptyMap();
```

</details>


<details>

<summary>Source Code for <code>NvdCveApiIndex.processDocuments(NvdVulnerabilityParser vulnerabilityParser, CveListData cveListData)</code></summary>

```java
final File[] files = super.getRequiredDownload(NvdCveApiDownload.class).listFiles(f -> f.isFile() && f.getName().endsWith(".json"));

if (files == null) {
    throw new RuntimeException("Unable to list files in " + super.getRequiredDownload(NvdCveApiDownload.class));
}

final Object WRITE_LOCK = new Object();
for (File file : files) {
    super.executor.submit(() -> {
        final Map<String, Document> documents = processNvdFile(file, vulnerabilityParser, cveListData);
        synchronized (WRITE_LOCK) {
            writeIndexDocuments(documents);
        }
    });
}

super.executor.setSize(6);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

if (cveListData != null) {
    final Set<String> unprocessed = cveListData.getUnprocessed();
    final Map<String, Document> documents = new HashMap<>();

    log.info("Found [{}] files in the CVEList data source that are unknown to the NVD, adding relevant to local index", unprocessed.size());
    for (String id : unprocessed) {
        try {
            final Vulnerability vulnerability = cveListData.getVulnerabilityFromFile(id, this.cveListParsingUtils.getParser());
            if (vulnerability != null) documents.put(vulnerability.getId(), vulnerability.toDocument());
        } catch (IOException e) {
            log.warn("Could not parse CVEList record");
        }
    }
    writeIndexDocuments(documents);
}
```

</details>


<p>The NVD CVE index is CVE-centric and contains all published CVEs from the NVD with their details, such as descriptions, references and matching information.
It is used in many steps of the enrichment process, whenever information on CVEs is required.
It is mainly used to match component product CPEs into CVEs as the base vulnerability information.</p>
<p>Every file contained in the download directory is parsed and processed in the same way. An entry in the parsed JSON
Array has these fields: <code>sourceIdentifier</code>, <code>references</code>, <code>configurations</code>, <code>weaknesses</code>, <code>id</code>, <code>published</code>,
<code>lastModified</code>, <code>metrics</code>, <code>vulnStatus</code>, <code>descriptions</code>.</p>
<table>
<caption>Mapping of JSON content to <code>Vulnerability</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">JSON</th>
<th style="text-align:left">Vulnerability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>descriptions</code> &gt; multiple <code>lang</code>/<code>value</code> --&gt; find <code>en</code> or fallback to any</td>
<td style="text-align:left">description</td>
</tr>
<tr>
<td style="text-align:left"><code>published</code></td>
<td style="text-align:left">create date</td>
</tr>
<tr>
<td style="text-align:left"><code>lastModified</code></td>
<td style="text-align:left">update date</td>
</tr>
<tr>
<td style="text-align:left"><code>references</code> &gt; multiple <code>source</code> &amp; <code>url</code> &amp; <code>tags</code></td>
<td style="text-align:left">references</td>
</tr>
<tr>
<td style="text-align:left"><code>weaknesses</code> &gt; multiple <code>description</code> &gt; multiple <code>value</code></td>
<td style="text-align:left">cwe</td>
</tr>
<tr>
<td style="text-align:left"><code>metrics</code> &gt; <code>cvssMetricV2</code> &gt; <code>cvssData</code> &gt; <code>vectorString</code></td>
<td style="text-align:left">cvss v2</td>
</tr>
<tr>
<td style="text-align:left"><code>metrics</code> &gt; <code>cvssMetricV30</code>, <code>cvssMetricV31</code> &gt; <code>cvssData</code> &gt; <code>vectorString</code></td>
<td style="text-align:left">cvss v3</td>
</tr>
<tr>
<td style="text-align:left"><code>metrics</code> &gt; <code>cvssMetricV40</code> &gt; <code>cvssData</code> &gt; <code>vectorString</code></td>
<td style="text-align:left">cvss v4</td>
</tr>
<tr>
<td style="text-align:left"><code>configurations</code> &gt; <code>nodes</code> (parse nodes)</td>
<td style="text-align:left">vulnerable software</td>
</tr>
</tbody>
</table>
<p>
More details can be found on the corresponding <a href="https://github.com/org-metaeffekt/metaeffekt-artifact-analysis/pull/336">Pull Request #336</a>.

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [NVD CVE API](download.md#nvd-cve-api) |
| **Optional Download** | [CVE List Git](download.md#cve-list-git) |
| **Optional Index** | [NVD CPE API Vendor Product](#nvd-cpe-api-vendor-product), [NVD CPE API](#nvd-cpe-api) |


## NVD CPE API

`cpe-dict` / `nvdCpeIndex`

<details>

<summary>Source Code for <code>NvdCpeApiIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

final File[] requiredDownloadFiles = super.getRequiredDownload(NvdCpeApiDownload.class).listFiles(f -> f.isFile() && f.getName().endsWith(".json"));
if (requiredDownloadFiles == null || requiredDownloadFiles.length == 0) {
    throw new RuntimeException("Unable to list files in " + super.getRequiredDownload(NvdCpeApiDownload.class));
}

final Index optionalCsafVulnIndex = super.getOptionalIndex(CsafVulnerabilityMatchingIndex.class);

final Map<String, Integer> statisticsNvdCounts = new ConcurrentHashMap<>();
for (File file : requiredDownloadFiles) {
    super.executor.submit(() -> {
        final Map<String, Document> results = processNvdFile(file);
        statisticsNvdCounts.put(file.getName().replace(".json", ""), results.size());
        synchronized (documents) {
            documents.putAll(results);
        }
    });
}

super.executor.setSize(6);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing of NVD CPEs to complete.", e);
}


final AtomicInteger statisticsCsafCounts = new AtomicInteger();
if (optionalCsafVulnIndex != null) {
    final IndexReader indexReader;
    try {
        indexReader = optionalCsafVulnIndex.getIndexReader();
    } catch (IOException e) {
        throw new RuntimeException("Failed to access CSAF Vulnerability Index", e);
    }

    for (int i = 0; i < indexReader.numDocs(); i++) {
        int finalI = i;
        super.executor.submit(() -> {
            final Map<String, Document> csafCpeDocuments = processCsaf(indexReader.document(finalI));
            statisticsCsafCounts.addAndGet(csafCpeDocuments.size());
            synchronized (documents) {
                documents.putAll(csafCpeDocuments);
            }
        });
    }

} else {
    LOG.info("No CSAF CPEs will be indexed, as the CSAF Vulnerability Index is not present");
}

super.executor.setSize(6);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing of CSAF CPEs to complete.", e);
}

// statistics
super.addStatistic(new StatisticBlock.StatsMetrics()
        .withTitle("Processing Summary")
        .withItem("Total CPEs", documents.size())
        .withItem("NVD", statisticsNvdCounts.values().stream().mapToInt(v -> v).sum())
        .withItem("CSAF", statisticsCsafCounts)
);

super.addStatistic(new StatisticBlock.StatsDistribution()
        .withTitle("NVD distribution")
        .withSegments(statisticsNvdCounts.entrySet().stream().sorted(Map.Entry.comparingByKey()).collect(Collectors.toList()),
                e -> Pair.of(e.getKey(), e.getValue().longValue()))
);

return documents;
```

</details>


<details>

<summary>Source Code for <code>NvdCpeApiIndex.processNvdFile(File file)</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

LOG.info("Processing file {}...", file.getAbsolutePath());

final JSONArray json;
try {
    json = new JSONArray(String.join("", FileUtils.readLines(file, StandardCharsets.UTF_8)));
} catch (IOException e) {
    throw new RuntimeException("Failed to read yearly NVD document from " + file.getAbsolutePath(), e);
}

LOG.info("Processing [{}] entries for file {}...", json.length(), file.getName());

for (int i = 0; i < json.length(); i++) {
    final JSONObject entry = json.getJSONObject(i);
    final String cpeName = entry.getString("cpeName");

    final Document document = CommonEnumerationUtil.parseCpe(cpeName)
            .map(NvdCpeApiIndex::createDocumentFromCpe)
            .orElse(null);

    if (document == null) {
        this.mirrorResult.addIncompleteStateMessage("Failed to parse CPE " + cpeName);
        continue;
    }

    final String cpeNameId = entry.getString("cpeNameId");
    final boolean deprecated = entry.getBoolean("deprecated");
    final JSONArray titles = entry.optJSONArray("titles");
    final JSONArray references = entry.optJSONArray("refs");

    final String lastModified = entry.getString("lastModified");
    final String created = entry.getString("created");

    document.add(new TextField("nvdId", cpeNameId, Field.Store.YES));
    document.add(new StringField("deprecated", Boolean.toString(deprecated), Field.Store.YES));
    if (titles != null) document.add(new TextField("titles", titles.toString(), Field.Store.YES));
    if (references != null) document.add(new TextField("references", references.toString(), Field.Store.YES));

    document.add(new StringField("lastModified", lastModified, Field.Store.YES));
    document.add(new StringField("created", created, Field.Store.YES));

    if (documents.containsKey(cpeNameId)) {
        LOG.warn("Duplicate CPE name id [{}]", cpeNameId);
    }
    documents.put(cpeNameId, document);
}
LOG.info("Completed processing file {}", file.getAbsolutePath());
return documents;
```

</details>


<details>

<summary>Source Code for <code>NvdCpeApiIndex.processCsaf(Document csafDocument)</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

CsafVulnerabilityEntry entry = CsafVulnerabilityEntry.fromDocument(csafDocument);
List<CsafProductIdentificationHelper> helpers = entry.getRelatedProducts().values().stream().flatMap(Collection::stream).collect(Collectors.toList());
for (CsafProductIdentificationHelper productHelper : helpers) {
    Cpe cpe = productHelper.getProductIdentifier().getCpe();

    AdvisoryTypeIdentifier<?> csafSourceType = AdvisoryTypeStore.get().fromNameWithoutCreation(csafDocument.get("source"));
    JSONObject references = new JSONObject().put("id", csafDocument.get("id"))
            .put("source", csafSourceType != null ? csafSourceType.getWellFormedName() : csafDocument.get("source"))
            .put("implementation", "CSAF");

    if (cpe != null) {
        synchronized (csafCpes) {
            if (!csafCpes.contains(cpe)) {
                Document documentFromCpe = createDocumentFromCpe(cpe);
                documentFromCpe.add(new TextField("references", references.toString(), Field.Store.YES));
                documents.put(UUID.randomUUID().toString(), documentFromCpe);
                csafCpes.add(cpe);
            }
        }
    }
}

return documents;
```

</details>


<p>This mirror contains all CPEs including all versions and their metadata that are known to the NVD.
It is mainly used in the CPE URI Derivation algorithm that attempts to match CPE to inventory components, and in the vulnerability timeline generator, as displayed in the Vulnerability Assessment Dashboard.</p>
<p>All entries from the API have been normalized to match the CPE Dictionary format in the according downloader step, which is why all CPE entries can be parsed in the same way.
All entries have at least these fields: <code>cpeName</code>, <code>cpeNameId</code>, <code>deprecated</code>, <code>titles</code>, <code>refs</code>, <code>lastModified</code> and <code>created</code></p>
<table>
<caption>Mapping of JSON content to <code>Cpe</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">JSON/XML</th>
<th style="text-align:left">data structure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>cpeName</code></td>
<td style="text-align:left"><code>cpe23Uri</code></td>
</tr>
<tr>
<td style="text-align:left">split CPE URI into parts</td>
<td style="text-align:left"><code>part</code>, <code>vendor</code>, <code>product</code>, <code>version</code>, <code>update</code>, <code>edition</code>, <code>language</code>, <code>sw_edition</code>, <code>target_sw</code>, <code>target_hw</code>, <code>other</code></td>
</tr>
<tr>
<td style="text-align:left"><code>cpeNameId</code></td>
<td style="text-align:left"><code>nvdId</code></td>
</tr>
<tr>
<td style="text-align:left"><code>deprecated</code></td>
<td style="text-align:left"><code>deprecated</code> (bool)</td>
</tr>
<tr>
<td style="text-align:left"><code>titles</code> &gt; multiple titles</td>
<td style="text-align:left"><code>titles</code> (JSON Array)</td>
</tr>
<tr>
<td style="text-align:left"><code>references</code> &gt; multiple refs</td>
<td style="text-align:left"><code>refs</code> (JSON Array)</td>
</tr>
</tbody>
</table>
<p>The NVD ID, titles, references and deprecated information can be queried later, where the titles and refs will be
properly parsed as language-specific titles and reference instances.</p>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [NVD CPE API](download.md#nvd-cpe-api) |
| **Optional Index** | [CSAF Vulnerability Matching](#csaf-vulnerability-matching) |


## NVD CPE API Vendor Product

`cpe-dict-vp` / `nvdCpeVendorProductIndex`

<details>

<summary>Source Code for <code>NvdCpeApiVendorProductIndex.createIndexDocuments()</code></summary>

```java
final NvdCpeApiIndexQuery indexQuery = new NvdCpeApiIndexQuery(getRequiredIndex(NvdCpeApiIndex.class));

final Map<String, Document> documents = new HashMap<>();

LOG.info("Parsing all [Vendor Products] / [Product Vendors] pairs");
final Map<String, Set<String>> vendorProductsMap = new HashMap<>();
final Map<String, Set<String>> productVendorsMap = new HashMap<>();

indexQuery.getIndex().findAndProcessAllDocuments(document -> {
    vendorProductsMap.computeIfAbsent(document.get("vendor"), k -> new HashSet<>()).add(document.get("product"));
    productVendorsMap.computeIfAbsent(document.get("product"), k -> new HashSet<>()).add(document.get("vendor"));
});
LOG.info("Extracted [{} Vendor Products] and [{} Product Vendors]", vendorProductsMap.size(), productVendorsMap.size());

LOG.info("Building Vendor Products map");
for (Map.Entry<String, Set<String>> vendorProducts : vendorProductsMap.entrySet()) {
    final StringJoiner productJoiner = new StringJoiner(", ");

    for (String product : vendorProducts.getValue()) {
        productJoiner.add(product);
    }

    final Document document = new Document();

    document.add(new StringField("type", "vp", Field.Store.YES));
    document.add(new TextField("vendor", vendorProducts.getKey(), Field.Store.YES));
    document.add(new TextField("product", productJoiner.toString(), Field.Store.YES));

    documents.put("vp" + vendorProducts.getKey(), document);
}

LOG.info("Building Product Vendors map");
for (Map.Entry<String, Set<String>> productVendors : productVendorsMap.entrySet()) {
    final StringJoiner vendorJoiner = new StringJoiner(", ");

    for (String vendor : productVendors.getValue()) {
        vendorJoiner.add(vendor);
    }

    final Document document = new Document();

    document.add(new StringField("type", "pv", Field.Store.YES));
    document.add(new TextField("product", productVendors.getKey(), Field.Store.YES));
    document.add(new TextField("vendor", vendorJoiner.toString(), Field.Store.YES));

    documents.put("pv" + productVendors.getKey(), document);
}

// statistics
final long totalRelations = vendorProductsMap.values().stream().mapToLong(Set::size).sum();

super.addStatistic(new StatisticBlock.StatsMetrics()
        .withTitle("Processing Summary")
        .withItem("Vendors", vendorProductsMap.size())
        .withItem("Products", productVendorsMap.size())
        .withItem("Pairs", totalRelations)
);

super.addStatistic(new StatisticBlock.StatsDistribution()
        .withTitle("Top Vendors")
        .withDescription("Vendors with the highest number of products (top 20).")
        .withShowPercentage(false).withSorted(false)
        .withSegments(
                vendorProductsMap.entrySet().stream()
                        .sorted((e1, e2) -> Integer.compare(e2.getValue().size(), e1.getValue().size()))
                        .limit(20)
                        .collect(Collectors.toList()),
                e -> Pair.of(e.getKey(), (long) e.getValue().size())
        )
);

super.addStatistic(new StatisticBlock.StatsDistribution()
        .withTitle("Top Products")
        .withDescription("Products provided by the highest number of vendors (top 20).")
        .withShowPercentage(false).withSorted(false)
        .withSegments(
                productVendorsMap.entrySet().stream()
                        .filter(e -> !e.getKey().equals("-") && !e.getKey().equals("*"))
                        .sorted((e1, e2) -> Integer.compare(e2.getValue().size(), e1.getValue().size()))
                        .limit(20)
                        .collect(Collectors.toList()),
                e -> Pair.of(e.getKey(), (long) e.getValue().size())
        )
);

return documents;
```

</details>


<p>For all vendors, store a set of their products.<br>For all products, store a set of their vendors.</p>
<p>This is used for quickly accessing all vendors/products of a product/vendor.
The information is extracted from the NVD CPE API index, making the index required to be completed before this one.</p>
<table>
<caption>Mapping of CSV content to <code>Document</code> fields</caption>
<thead>
<tr>
<th style="text-align:left"><strong>index</strong></th>
<th style="text-align:left"><strong>data structure</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">if <code>vendor → products</code>: <code>vp</code><br>else if <code>product → vendors</code>: <code>pv</code></td>
<td style="text-align:left"><code>type</code></td>
</tr>
<tr>
<td style="text-align:left">if <code>vp</code>: a single vendor<br>else if pv: a CSV list of vendors for the product</td>
<td style="text-align:left"><code>vendor</code></td>
</tr>
<tr>
<td style="text-align:left">if <code>pv</code>: a single product<br>else if vp: a CSV list of products for the vendor</td>
<td style="text-align:left"><code>product</code></td>
</tr>
</tbody>
</table>
<p>Note that this process is (as of now) 100% identical to the deprecated CPE Dictionary Vendor/Product Index, as the CPE
Dictionary and NVD CPE API interface are the same.</p>

| Dependency Type | Depends on |
| --- | --- |
| **Index** | [NVD CPE API](#nvd-cpe-api) |


## MSRC Product

`msrc-products` / `msrcProductIndex`

<details>

<summary>Source Code for <code>MsrcProductIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final List<File> files = super.getAllFilesInSubDirectories(super.getRequiredDownload(MsrcDownload.class));

final Set<String> productIdsWithFullContent = new HashSet<>();
final Map<String, Set<String>> cpes = new HashMap<>();

for (File file : files) {
    if (file.getName().endsWith(".xml")) {
        super.executor.submit(() -> {
            final List<MsrcProduct> products = parseMsProductsFromFile(file);

            for (MsrcProduct product : products) {
                if (product.getCpes().isEmpty()) continue;
                cpes.computeIfAbsent(product.getId(), key -> new HashSet<>()).addAll(product.getCpes());
            }
            for (MsrcProduct product : products) {
                if (productIdsWithFullContent.contains(product.getId())) {
                    continue;
                } else if (product.getVendor() != null && product.getFamily() != null) {
                    productIdsWithFullContent.add(product.getId());
                }

                documents.put(product.getId(), product.toDocument());
            }
        });
    }
}

super.executor.setSize(16);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

cpes.forEach((key, entry) -> {
    Document document = documents.get(key);
    JSONArray cpesFromDocument = new JSONArray(document.get("cpes"));
    entry.forEach(cpesFromDocument::put);
    document.removeField("cpes");
    document.add(new TextField("cpes", cpesFromDocument.toString(), Field.Store.YES));
});

return documents;
```

</details>


<details>

<summary>Source Code for <code>MsrcProductIndex.parseMsProductsFromFile(File file)</code></summary>

```java
final List<MsrcProduct> products = new ArrayList<>();

final List<String> lines;
try {
    lines = FileUtils.readLines(file, StandardCharsets.UTF_8);
} catch (IOException e) {
    throw new RuntimeException("Cannot read file to extract ms products " + file.getAbsolutePath(), e);
}

if (lines.isEmpty() || lines.size() < 3 && StringUtils.isEmpty(String.join("", lines))) {
    return products;
}

final org.w3c.dom.Document xmlDocument;
try {
    xmlDocument = parseXmlDocument(String.join("", lines));
} catch (ParserConfigurationException | IOException | SAXException | IllegalArgumentException e) {
    throw new RuntimeException("Cannot parse xml file " + file.getAbsolutePath(), e);
}

// extract product name, product id, product family, vendor
final NodeList productNodes = xmlDocument.getElementsByTagName("prod:FullProductName");

for (int i = 0; i < productNodes.getLength(); i++) {
    final Element productElement = (Element) productNodes.item(i);

    final String productId = productElement.getAttribute("ProductID");
    final Set<String> productCpe = new HashSet<>();
    final String cpe = productElement.getAttribute("CPE");
    if (!cpe.isEmpty()) productCpe.add(cpe);
    final String productName = productElement.getTextContent();

    final Node productFamilyNameNode = productElement.getParentNode().getAttributes().getNamedItem("Name");
    final String productFamily = productFamilyNameNode == null ? null : productFamilyNameNode.getTextContent();

    final Node vendorNameNode = productElement.getParentNode().getParentNode().getAttributes().getNamedItem("Name");
    final String vendor = vendorNameNode == null ? null : vendorNameNode.getTextContent();

    products.add(new MsrcProduct(productId, productName, productCpe, productFamily, vendor));
}

return products;
```

</details>


<p>When downloading the Microsoft Security Response Center data (MSRC), a list of affected products is included. The
majority of these products are developed by Microsoft. Each monthly file within the download contains only the products
that are affected by the advisories in that particular file. As a result, there may be multiple files with the same
product, each containing varying amounts of information. To generate a comprehensive list, all files must be parsed.</p>
<p>The product tree consists of two layers: vendors &gt; multiple products with product families. Not all products have a
family, some are listed on the upper layer.</p>
<p>The product tree is organized into two layers: vendors and multiple products with product families. Most products are
child nodes of the vendors, but not all products have a family/vendor: some are listed on the upper layer directly.</p>
<table>
<caption>Mapping of XML content to <code>MsrcProduct</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">XML</th>
<th style="text-align:left">data structure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>FullProductName</code> &gt; attribute <code>ProductID</code></td>
<td style="text-align:left"><code>id</code></td>
</tr>
<tr>
<td style="text-align:left"><code>FullProductName</code></td>
<td style="text-align:left"><code>name</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Branch</code> &gt; where <code>Type=&quot;Product Family&quot;</code> &gt; <code>Name</code></td>
<td style="text-align:left"><code>family</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Branch</code> &gt; where <code>Type=&quot;Vendor&quot;</code> &gt; <code>Name</code></td>
<td style="text-align:left"><code>vendor</code></td>
</tr>
</tbody>
</table>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [MSRC](download.md#msrc) |


## MSRC Advisor

`msrc-advisors` / `msrcAdvisorIndex`

<details>

<summary>Source Code for <code>MsrcAdvisorIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final List<File> files = super.getAllFilesInSubDirectories(super.getRequiredDownload(MsrcDownload.class));

nvdEquivalentStatistics.set(0);
for (File file : files) {
    if (file.getName().endsWith(".xml")) {
        super.executor.submit(() -> {
            try {
                final String lines = FileUtils.readFileToString(file, StandardCharsets.UTF_8);

                if (StringUtils.isEmpty(lines)) {
                    return;
                }

                final org.w3c.dom.Document document = parseXmlDocument(lines);

                final List<MsrcAdvisorEntry> parsedEntries = MsrcAdvisorEntry.fromDownloadXml(document);

                for (MsrcAdvisorEntry entry : parsedEntries) {
                    // if an advisory has been updated across the years (files), the old files are no longer updated and a duplicate is added to the new files
                    if (documents.containsKey(entry.getId())) {
                        entry.appendFromDataClass(MsrcAdvisorEntry.fromDocument(documents.get(entry.getId())));
                    }

                    this.checkIsNvdEquivalent(entry);

                    documents.put(entry.getId(), entry.toDocument());
                }
            } catch (IOException e) {
                throw new RuntimeException("Unable to read file contents during indexing: " + file.getAbsolutePath(), e);
            } catch (Exception e) {
                throw new RuntimeException("Unable to parse file content during indexing: " + file.getAbsolutePath(), e);
            }
        });
    }
}

super.executor.setSize(16);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}
LOG.info("[{}] entries are considered NVD Equivalent", nvdEquivalentStatistics);

return documents;
```

</details>


<details>

<summary>Source Code for <code>MsrcAdvisorEntry.fromDownloadXml(org.w3c.dom.Document document)</code></summary>

```java
final List<MsrcAdvisorEntry> entries = new ArrayList<>();

final NodeList vulnerabilityElements = document.getElementsByTagName("vuln:Vulnerability");

for (int i = 0; i < vulnerabilityElements.getLength(); i++) {
    final Element vulnerabilityElement = (Element) vulnerabilityElements.item(i);

    final MsrcAdvisorEntry entry = fromDownloadXmlVulnerabilityElement(vulnerabilityElement);

    entries.add(entry);
}

return entries;
```

</details>


<p>The advisories provided in the MSRC download are each almost always linked to at least one CVE, which is also their ID.
Very rarely however, the ID may be an <code>ADV\d+</code> identifier, that is not related a specific CVE.</p>
<p>Some of the AdvisorEntry-related fields such as workarounds, etc. cannot be filled out, as they relate to a specific
product and are stored in separate fields.</p>
<table>
<caption>Mapping of JSON content to <code>MsrcAdvisorEntry</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">XML</th>
<th style="text-align:left">MsrcAdvisorEntry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>CVE</code> (with <code>MSRC</code> prefix)</td>
<td style="text-align:left"><code>id</code></td>
</tr>
<tr>
<td style="text-align:left"><code>RevisionHistory</code> &gt; first <code>Revision</code> &gt; <code>Date</code></td>
<td style="text-align:left"><code>createDate</code></td>
</tr>
<tr>
<td style="text-align:left"><code>RevisionHistory</code> &gt; latest <code>Revision</code> &gt; <code>Date</code></td>
<td style="text-align:left"><code>updateDate</code></td>
</tr>
<tr>
<td style="text-align:left">all CVE-IDs in the note elements</td>
<td style="text-align:left"><code>referenceIds</code></td>
</tr>
<tr>
<td style="text-align:left"><code>CVSSScoreSets</code> &gt; each <code>ScoreSet</code> &gt; <code>Vector</code> &amp; <code>ProductID</code></td>
<td style="text-align:left"><code>productCvssVectors</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Threats</code> &gt; each <code>Threat</code> &gt; <code>Type</code> &amp; <code>Description</code> &amp; <code>ProductID</code></td>
<td style="text-align:left"><code>msThreats</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Remediations</code> &gt; each <code>Remediation</code> &gt; multiple tags</td>
<td style="text-align:left"><code>msRemediations</code></td>
</tr>
<tr>
<td style="text-align:left"><code>ProductStatuses</code> &gt; each <code>Status</code> &gt; <code>ProductID</code></td>
<td style="text-align:left"><code>affectedProducts</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Title</code></td>
<td style="text-align:left"><code>summary</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Notes</code> &gt; each <code>Note</code> &gt; (<code>Type</code> &amp; <code>Title</code>) &amp; (Text content)</td>
<td style="text-align:left"><code>description</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Threats</code> &gt; first <code>Threat</code> without <code>ProductId</code></td>
<td style="text-align:left"><code>threat</code></td>
</tr>
</tbody>
</table>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [MSRC](download.md#msrc) |
| **Index** | [NVD CVE API](#nvd-cve-api) |


## MSRC KB Chain

`msrc-kb-chains` / `msrcKbChainIndex`

<details>

<summary>Source Code for <code>MsrcKbChainIndex.createIndexDocuments()</code></summary>

```java
final MsrcAdvisorIndexQuery advisorQuery = new MsrcAdvisorIndexQuery(super.getRequiredIndex(MsrcAdvisorIndex.class));
final MsrcProductIndexQuery productQuery = new MsrcProductIndexQuery(super.getRequiredIndex(MsrcProductIndex.class));

final List<File> allMsrcUpdateGuideCsvFiles = getAllMsrcUpdateGuideCsvFiles();
final List<File> allMsrcUpdateGuideJsonFiles = getAllMsrcUpdateGuideJsonFiles();

LOG.info("");
LOG.info("Parsing KB nodes from:");
LOG.info("  - the MSRC Api Advisor Mirror");
if (!allMsrcUpdateGuideCsvFiles.isEmpty()) {
    // total entries as of 2023-02-28: 114781
    LOG.info("  - [{}] MSRC Security Update Guide CSV files", allMsrcUpdateGuideCsvFiles.size());
}
if (!allMsrcUpdateGuideJsonFiles.isEmpty()) {
    // total entries as of 2023-07-11: 127517
    LOG.info("  - [{}] MSRC Security Update Guide JSON files", allMsrcUpdateGuideJsonFiles.size());
}
LOG.info("");
LOG.info("- - - - - - - -");


// from the MSRC Api Advisor Mirror
final List<MsrcSupersedeNode> msrcApiNodes = createMsrcApiNodes(advisorQuery);

// from the MSRC Security Update Guide CSV files
final Map<File, List<MsrcSupersedeNode>> msrcUpdateGuideNodes = createMsrcUpdateGuideNodes(productQuery, allMsrcUpdateGuideCsvFiles);

// from the MSRC Security Update Guide JSON files
final Map<File, List<MsrcSupersedeNode>> msrcUpdateGuideJsonNodes = createMsrcUpdateGuideJsonNodes(productQuery, allMsrcUpdateGuideJsonFiles);


// combine the lists into a single list
final List<MsrcSupersedeNode> parsedNodes = new ArrayList<>(msrcApiNodes);
for (Map.Entry<File, List<MsrcSupersedeNode>> fileEntry : msrcUpdateGuideNodes.entrySet()) {
    parsedNodes.addAll(fileEntry.getValue());
}
for (Map.Entry<File, List<MsrcSupersedeNode>> fileEntry : msrcUpdateGuideJsonNodes.entrySet()) {
    parsedNodes.addAll(fileEntry.getValue());
}

// merge the nodes together
final Map<String, MsrcSupersedeNode> uniqueNodes = MsrcSupersedeNode.mergeNodes(Collections.singleton(parsedNodes));
LOG.info("Deduplicated parsed KB entries [{} --> {}]", parsedNodes.size(), uniqueNodes.size());


// create documents
final Map<String, Document> documents = new HashMap<>();
for (Map.Entry<String, MsrcSupersedeNode> entry : uniqueNodes.entrySet()) {
    documents.put(entry.getKey(), entry.getValue().toDocument());
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>MsrcKbChainIndex.createMsrcApiNodes(MsrcAdvisorIndexQuery advisorQuery)</code></summary>

```java
final Map<String, MsrcSupersedeNode> nodes = new HashMap<>();

LOG.info("Querying MSRC API for KB entries");

for (MsrcAdvisorEntry entry : advisorQuery.findAll()) {
    final String vulnerabilityId = entry.getId().replace("MSRC-", "");

    for (MsrcRemediation msRemediation : entry.getMsrcRemediations()) {
        final String description = msRemediation.getDescription();

        if (isKbIdentifier(description)) {
            final MsrcSupersedeNode node = nodes.computeIfAbsent(description, MsrcSupersedeNode::new);

            final Set<String> affectedProductIds = msRemediation.getAffectedProductIds();
            final String supercedence = msRemediation.getSupercedence();
            final List<String> supersedence = extractSupersedeIdentifiers(supercedence);

            for (String affectedProductId : affectedProductIds) {
                node.addAffectsVulnerability(affectedProductId, vulnerabilityId);

                for (String supersededKbId : supersedence) {
                    final MsrcSupersedeNode supersededNode = nodes.computeIfAbsent(supersededKbId, MsrcSupersedeNode::new);
                    node.addSupersedes(affectedProductId, supersededNode);
                    supersededNode.addSupersededBy(affectedProductId, node);
                }
            }
        }
    }
}

final Map<String, MsrcSupersedeNode> normalized = MsrcSupersedeNode.mergeNodes(Collections.singletonList(nodes.values()));

if (normalized.size() != nodes.size()) {
    LOG.info("Found [{}] --> [{}] KB entries", nodes.size(), normalized.size());
} else {
    LOG.info("Found [{}] KB entries", nodes.size());
}
LOG.info("- - - - - - - -");

return new ArrayList<>(normalized.values());
```

</details>


<details>

<summary>Source Code for <code>MsrcKbChainIndex.createMsrcUpdateGuideNodes(MsrcProductIndexQuery productQuery, List files)</code></summary>

```java
final Map<File, List<MsrcSupersedeNode>> msrcUpdateGuideNodes = new LinkedHashMap<>();
if (files.isEmpty()) {
    return msrcUpdateGuideNodes;
}

for (File msrcUpdateGuideDownloadCsvFile : files) {
    super.executor.submit(() -> {
        try {
            final List<MsrcSupersedeNode> msrcCsvDownloadNodes = createMsrcCsvDownloadNodes(productQuery, msrcUpdateGuideDownloadCsvFile);
            msrcUpdateGuideNodes.put(msrcUpdateGuideDownloadCsvFile, msrcCsvDownloadNodes);
        } catch (IOException e) {
            LOG.error("Failed to read MSRC update guide CSV file: {}", msrcUpdateGuideDownloadCsvFile, e);
            throw new RuntimeException("Failed to read MSRC update guide CSV file: " + msrcUpdateGuideDownloadCsvFile, e);
        } catch (Exception e) {
            LOG.error("Failed to parse MSRC update guide CSV file: {}", msrcUpdateGuideDownloadCsvFile, e);
            throw new RuntimeException("Failed to parse MSRC update guide CSV file: " + msrcUpdateGuideDownloadCsvFile, e);
        }
    });
}

super.executor.setSize(16);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}
LOG.info("- - - - - - - -");

return msrcUpdateGuideNodes;
```

</details>


<details>

<summary>Source Code for <code>MsrcKbChainIndex.createMsrcJsonDownloadNodes(MsrcProductIndexQuery productQuery, File file)</code></summary>

```java
if (file.isDirectory() || !file.isFile() || !file.getName().endsWith(".json")) {
    LOG.warn("File is not a JSON file, but a directory: " + file.getAbsolutePath());
    return Collections.emptyList();
}

LOG.info("Parsing JSON file from [{}]", file.getAbsolutePath());

final JSONArray jsonArray = new JSONArray(FileUtils.readFileToString(file, StandardCharsets.UTF_8));
LOG.info("Parsing JSON file with [{}] entries", jsonArray.length());

final Map<String, MsrcSupersedeNode> nodes = new HashMap<>();
final Set<String> unknownProducts = new HashSet<>();

for (int i = 0; i < jsonArray.length(); i++) {
    final JSONObject entry = jsonArray.getJSONObject(i);
    final String cveId = entry.getString("cveNumber");
    final JSONArray entryArticles = entry.getJSONArray("kbArticles");

    final String productName = entry.getString("product");
    final String effectiveProduct = resolveEffectiveProduct(productQuery, productName, unknownProducts, cveId);

    for (int j = 0; j < entryArticles.length(); j++) {
        final JSONObject currentArticle = entryArticles.getJSONObject(j);
        final String article = currentArticle.optString("articleName", null);
        final String articleUrl = ObjectUtils.firstNonNull(currentArticle.optString("articleUrl", null), currentArticle.optString("knownIssuesUrl", null));
        final String downloadUrl = currentArticle.optString("downloadUrl", null);

        if (StringUtils.hasText(cveId) && StringUtils.hasText(article)) {
            if (article.matches("\\d{6,8}")) {
                for (String product : effectiveProduct.split(",")) {
                    nodes.computeIfAbsent(article, a -> new MsrcSupersedeNode(a, articleUrl, downloadUrl))
                            .addAffectsVulnerability(product, cveId);
                }
            }
        }
    }
}

LOG.info("Parsed [{}] KB entries", nodes.size());

return new ArrayList<>(MsrcSupersedeNode.mergeNodes(Collections.singletonList(nodes.values())).values());
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | [MSRC Security Guide](download.md#msrc-security-guide) |
| **Index** | [MSRC Advisor](#msrc-advisor), [MSRC Product](#msrc-product) |
| **Optional Download** | _MSRC Manual CSV_ |


## OSV Advisor

`osv-advisory-database` / `osvAdvisorIndex`

<details>

<summary>Source Code for <code>OsvAdvisorIndex.createIndexDocuments()</code></summary>

```java
// just always show because GLOBAL_ECOSYSTEM_BLACKLIST cannot be empty right now
log.info("Exclude advisories policy:");
if (!GLOBAL_ECOSYSTEM_BLACKLIST.isEmpty()) log.info("- Ecosystems: {}", GLOBAL_ECOSYSTEM_BLACKLIST);
if (!excludeAdvisoryProviders.isEmpty()) log.info("- Providers:  {}", excludeAdvisoryProviders);

log.info("Searching for files in OSV, this may take a while: {}", super.getRequiredDownload(OsvDownload.class).getAbsolutePath());
final Collection<File> osvFiles = super.getAllFilesRecursively(super.getRequiredDownload(OsvDownload.class)).stream()
        .filter(file -> !GLOBAL_ECOSYSTEM_BLACKLIST.contains(file.getName().split("-")[0]))
        .collect(Collectors.toList());
log.info("Found [{}] files in OSV", osvFiles.size());

final Map<String, File> knownFiles = new HashMap<>();
for (File file : osvFiles) {
    getLastModified(file, knownFiles);
}
log.info("Found [{}] unique files in OSV", knownFiles.size());

int ghsaFileCount = 0;
if (super.getOptionalDownload(GhsaDownload.class) != null) {
    log.info("Searching for files in GHSA repository, this may take a while: {}", super.getOptionalDownload(GhsaDownload.class).getAbsolutePath());
    final Collection<File> ghsaFiles = super.getAllFilesRecursively(super.getOptionalDownload(GhsaDownload.class));
    ghsaFileCount = ghsaFiles.size();
    log.info("Found [{}] files in GHSA repository", ghsaFileCount);

    for (File file : ghsaFiles) {
        getLastModified(file, knownFiles);
    }
    log.info("Found [{}] unique files in OSV and GHSA repository", knownFiles.size());
}

this.totalFiles = knownFiles.size();

log.info("Starting processing of files batches");
Lists.partition(new ArrayList<>(knownFiles.values()), 50000)
        .forEach(this::parseFiles);

if (!genericAdvisoryIds.isEmpty()) {
    log.warn("Finished processing all files. Found [{}] total unrecognized OSV advisory Ids: {}",
            genericAdvisoryIds.size(), String.join(", ", genericAdvisoryIds));
}

log.info("Processed all [{}] files; [{} NVD equivalent] [{} excluded]", knownFiles.size(), this.nvdEquivalentStatistics, this.excludeAdvisoriesStatistics);

// statistics
super.addStatistic(new StatsMetrics()
        .withTitle("Processing Summary")
        .withItem("Total Files", knownFiles.size())
        .withItem("OSV Files", osvFiles.size())
        .withItem("GHSA Files", ghsaFileCount)
        .withItem("NVD Equivalent", this.nvdEquivalentStatistics.get())
        .withItem("Excluded", this.excludeAdvisoriesStatistics.get()));

final StatsDistribution distribution = super.addStatistic(new StatsDistribution()
        .withTitle("Advisories by Provider")
        .withSorted(true));
this.providerCounts.entrySet().stream()
        .sorted(Map.Entry.comparingByKey(Comparator.comparing(AdvisoryTypeIdentifier::getWellFormedName)))
        .forEach(entry -> distribution.withSegment(entry.getKey().getWellFormedName(), entry.getValue().get()));

if (!this.excludeAdvisoryProviders.isEmpty()) {
    final StatsList excludedList = new StatsList()
            .withTitle("Excluded Providers");
    this.excludeAdvisoryProviders.stream()
            .map(AdvisoryTypeIdentifier::getWellFormedName)
            .sorted()
            .forEach(excludedList::withItem);
    super.addStatistic(excludedList);
}

// we do our own writing in parsing methods
return Collections.emptyMap();
```

</details>


<details>

<summary>Source Code for <code>OsvAdvisorIndex.parseFiles(Collection files)</code></summary>

```java
final Map<String, Document> toWrite = files.stream()
        .filter(file -> file.getName().endsWith(JSON_EXTENSION))
        .parallel()
        .map((file) -> {
            final String path = file.getAbsolutePath();
            final String originEcosystem = file.getParentFile().getName();

            try {
                final OsvAdvisorEntry parsedEntry;
                try {
                    parsedEntry = OsvDataParser.fromOsvDownloadFile(file, originEcosystem);
                    if (parsedEntry.getSourceIdentifier() == AdvisoryTypeStore.OSV_GENERIC_IDENTIFIER) {
                        genericAdvisoryIds.add(parsedEntry.getId());
                    }
                } catch (Exception e) {
                    log.error("Unable to parse file content during indexing: {}\n{}", path, e);
                    throw e;
                }

                // statistics
                this.providerCounts
                        .computeIfAbsent(parsedEntry.getSourceIdentifier(), k -> new AtomicInteger(0))
                        .incrementAndGet();

                if (isExcluded(parsedEntry)) {
                    excludeAdvisoriesStatistics.incrementAndGet();
                    return null;
                }

                this.checkIsNvdEquivalent(parsedEntry);

                return new AbstractMap.SimpleImmutableEntry<>(parsedEntry.getId(), parsedEntry.toDocument());

            } catch (IOException e) {
                throw new RuntimeException("Unable to read file contents during indexing: " + path, e);
            } catch (Exception e) {
                throw new RuntimeException("Unable to parse file content during indexing: " + path, e);
            }
        })
        .filter(Objects::nonNull)
        .collect(Collectors.toConcurrentMap(AbstractMap.SimpleImmutableEntry::getKey, AbstractMap.SimpleImmutableEntry::getValue));

final int currentProcessed = this.processedFilesStatistics.addAndGet(files.size());
final int percentage = (int) ((double) currentProcessed / this.totalFiles * 100);

log.info("[{}/{}] [{}%] Processed batch of [{}] files, writing into index; [{} NVD equivalent] [{} excluded]",
        currentProcessed, this.totalFiles, percentage, files.size(), nvdEquivalentStatistics, excludeAdvisoriesStatistics);
writeIndexDocuments(toWrite);
```

</details>


<details>

<summary>Source Code for <code>OsvDataParser.fromOsvDownloadJson(JSONObject json, String originEcosystem)</code></summary>

```java
final String prefix = json.optString("id");

final AdvisoryTypeIdentifier<?> foundIdentifier = guessAdvisoryProviderFromId(prefix);

// logging about unknown providers is handled by the OsvAdvisoryIndex
/*if (foundIdentifier == AdvisoryTypeStore.OSV_GENERIC_IDENTIFIER) {
    log.warn("No matching AdvisoryTypeIdentifier found for ID: {}", prefix);
}*/

final OsvAdvisorEntry entry = new OsvAdvisorEntry(foundIdentifier);

if (hasText(originEcosystem) && originEcosystem.startsWith("GHSA-")) {
    entry.setOriginEcosystem("GHSA");
} else {
    entry.setOriginEcosystem(originEcosystem);
}

OsvDataParser.parseBasicInfo(entry, json);
OsvDataParser.parseReferencedVulnerabilities(entry, json);
OsvDataParser.parseSeverity(entry, json);
OsvDataParser.parseAffected(entry, json);
OsvDataParser.parseReferences(entry, json);
OsvDataParser.parseDatabaseSpecific(entry, json);

if (prefix.startsWith("CVE"))
    entry.addReferencedSecurityAdvisory(AdvisoryTypeStore.OSV_CVE, prefix);

return entry;
```

</details>


<p>References:</p>
<ul>
<li>Open Source Vulnerability Database: <a href="https://google.github.io/osv.dev/">OSV</a></li>
<li>GHSA: <a href="https://github.com/advisories">https://github.com/advisories</a></li>
</ul>
<p>The Open Source Vulnerability Advisories are available as JSON files. Each file contains a single advisory, which is
identified by its ID. The ID is also used as the filename.</p>
<p>A speciality with this data source is that it provides matching information, containing the ecosystem, package name and
version range for each affected package. This information is stored in the <code>affected</code> field, which is an array of
objects.</p>
<pre><code class="lang-json">
"affected": [
  {
    "package": {
      "ecosystem": "Packagist",
      "name": "francoisjacquet/rosariosis"
    },
    "ranges": [
      {
        "type": "ECOSYSTEM",
        "events": [
          {
            "introduced": "0"
          },
          {
            "fixed": "8.9.3"
          }
        ]
      }
    ]
  }
]
</code></pre>
<p>Also unique to the OSV-Schema, the <code>database-specifc</code> field contains additional information about the
vulnerability as defined by the ecosystem&#39;s database which the record was obtained.</p>
<pre><code class="lang-json">
"database_specific": {
    "cwe_ids": [
      "CWE-79"
    ],
    "severity": "MODERATE",
    "github_reviewed": true,
    "github_reviewed_at": "2022-09-16T18:41:43Z",
    "nvd_published_at": "2022-09-01T08:15:00Z"
}
</code></pre>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [OSV](download.md#osv) |
| **Index** | [NVD CVE API](#nvd-cve-api) |
| **Optional Download** | [GHSA](download.md#ghsa) |


## CSAF Vulnerability Matching

`csaf-vuln` / `csafVulnerabilityIndex`

<details>

<summary>Source Code for <code>CsafVulnerabilityMatchingIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

for (Map.Entry<String, List<File>> entry : getYearDirs(super.getRequiredDownload(CsafDownload.class)).entrySet()) {
    for (File yearDir : entry.getValue()) {
        super.executor.submit(() -> {
            final Collection<File> files = FileUtils.listFiles(yearDir, new String[]{"json"}, false);

            log.info("[{} / {}] Processing [{}] files", entry.getKey(), yearDir.getName(), files.size());
            int totalDocuments = 0;
            int i = 0;
            for (File csafFile : files) {
                i++;
                final Map<String, Document> parsedDocument = processFile(csafFile);
                totalDocuments += parsedDocument.size();
                synchronized (documents) {
                    documents.putAll(parsedDocument);
                }
                final Map<String, Document> shallowCopy;
                synchronized (documents) {
                    if (documents.size() >= 5000) {
                        log.info("[{} / {}] [{} / {}] Writing [{}] documents into index", entry.getKey(), yearDir.getName(), i, files.size(), documents.size());
                        shallowCopy = new HashMap<>(documents);
                        documents.clear();
                    } else {
                        shallowCopy = null;
                    }
                }
                if (shallowCopy != null) {
                    synchronized (LUCENE_INDEX_WRITE_LOCK) {
                        writeIndexDocuments(shallowCopy);
                    }
                }
            }
            log.info("[{} / {}] Found [{}] vulnerability document entries while processing [{}] documents",
                    entry.getKey(), yearDir.getName(), totalDocuments, files.size());
        });
    }
}

super.executor.setSize(8);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

return documents;
```

</details>


Index for CSAF vulnerability matching<br>
Fields:
<p>
<b>id</b>: original document name<br>
<b>source</b>: producer of CSAF document<br>
<b>vulnerability</b>: vulnerability ID (e.g. CVE)<br>
<b>related_products</b>: all product information related to vulnerability defined in document, is compressed before written into index<br>
<b>keywords</b>: keywords of purl or cpe inorder to improve searching of index<br>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CSAF](download.md#csaf) |


## CSAF Advisory

`csaf-advisors` / `csafAdvisoryIndex`

<details>

<summary>Source Code for <code>CsafAdvisoryIndex.createIndexDocuments()</code></summary>

```java
for (Map.Entry<String, List<File>> entry : getYearDirs(super.getRequiredDownload(CsafDownload.class)).entrySet()) {
    for (File year : entry.getValue()) {
        super.executor.submit(() -> {
            log.info("[{} / {}] Extracting CSAF Advisories from provider", entry.getKey(), year.getName());
            int totalDocuments = 0;
            final Map<String, Document> hold = new HashMap<>();
            final File[] files = year.listFiles(File::isFile);
            int i = 0;
            for (File csafFile : files) {
                i++;
                try {
                    final CsafAdvisoryEntry advisoryEntry = CsafAdvisoryEntry.fromCsafEntry(CsafEntry.fromCsafJson(csafFile));
                    hold.put(advisoryEntry.getId(), advisoryEntry.toDocument());
                    totalDocuments++;
                    if (totalDocuments >= 1000) {
                        totalDocuments = 0;
                        synchronized (LUCENE_INDEX_WRITE_LOCK) {
                            log.info("[{} / {}] [{} / {}] Writing [{}] documents into index", entry.getKey(), year.getName(), i, files.length, hold.size());
                            writeIndexDocuments(hold);
                            hold.clear();
                        }
                    }
                } catch (Exception e) {
                    log.error("[{} / {}] Failed to parse CSAF Advisory entry, skipping: {}", entry.getKey(), year.getName(), csafFile.getAbsolutePath(), e);
                }
            }
            synchronized (LUCENE_INDEX_WRITE_LOCK) {
                writeIndexDocuments(hold);
            }
        });
    }
}

super.executor.setSize(8);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

return new HashMap<>();
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CSAF](download.md#csaf) |


## CERT-SEI Advisor

`certsei-advisors` / `certSeiAdvisorIndex`

<details>

<summary>Source Code for <code>CertSeiAdvisorIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

final List<File> files = super.getAllFilesInSubDirectories(super.getRequiredDownload(CertSeiDownload.class));

for (File file : files) {
    try {
        final List<String> contents = FileUtils.readLines(file, StandardCharsets.UTF_8);
        final JSONObject jsonContent = new JSONObject(String.join("", contents));

        final CertSeiAdvisorEntry parsedEntry = CertSeiAdvisorEntry.fromDownloadJson(jsonContent);

        documents.put(parsedEntry.getId(), parsedEntry.toDocument());
    } catch (IOException e) {
        throw new RuntimeException("Unable to read file contents during indexing ", e);
    }
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>CertSeiAdvisorEntry.fromDownloadJson(JSONObject json)</code></summary>

```java
final CertSeiAdvisorEntry entry = new CertSeiAdvisorEntry();

{
    entry.setId(firstNonEmpty(
            json.optString("vuid", null),
            json.optString("idnumber", null),
            json.optString("id", null),
            "unknown"
    ).toUpperCase(Locale.ENGLISH)
            .replaceAll("[-–_ ]+", "-")
            .replaceAll("\\D*(\\d+)\\D*", "VU#$1"));
}

{
    entry.setCreateDate(TimeUtils.tryParse(firstNonEmpty(
            json.optString("datecreated", null),
            json.optString("datefirstpublished", null),
            json.optString("publicdate", null),
            json.optString("dateupdated", null)
    )));
    entry.setUpdateDate(TimeUtils.tryParse(firstNonEmpty(
            json.optString("dateupdated", null),
            json.optString("datecreated", null),
            json.optString("datefirstpublished", null),
            json.optString("publicdate", null)
    )));
}

{
    final JSONArray referencesJson = json.optJSONArray("public");
    if (referencesJson != null) {
        final StringBuilder brokenUrlCollector = new StringBuilder();

        for (int i = 0; i < referencesJson.length(); i++) {
            if (StringUtils.hasText(referencesJson.optString(i))) {
                if (referencesJson.getString(i).length() == 1) {
                    brokenUrlCollector.append(referencesJson.getString(i));
                } else {
                    entry.addReference(Reference.fromUrl(referencesJson.optString(i)));
                }
            }
        }

        if (brokenUrlCollector.length() > 0) {
            entry.addReference(Reference.fromUrl(brokenUrlCollector.toString()));
        }
    }
}

{
    final JSONArray keywordsJson = json.optJSONArray("keywords");
    if (keywordsJson != null) {
        for (int i = 0; i < keywordsJson.length(); i++) {
            entry.addKeyword(keywordsJson.optString(i));
        }
    }
}

{
    final Map<String, String> overviewMap = new HashMap<>();

    if (json.optJSONObject("overview") != null) {
        LOG.info("overview is JSON on {}", entry.getId());
        json.optJSONObject("overview").toMap()
                .forEach((key, value) -> overviewMap.put(key, String.valueOf(value)));
    } else {
        if (json.has("overview")) {
            final String overviewString = String.valueOf(json.opt("overview"));

            String currentTitle = "";
            StringJoiner currentLines = new StringJoiner("\n");

            final String[] lines = overviewString.split("\r?\n");

            for (String line : lines) {
                final String normalizedTitle = certSeiNormalizeTitleIfAvailable(line);

                if (normalizedTitle != null) {
                    if (currentLines.length() > 0) {
                        overviewMap.put(currentTitle, trimNewlines(currentLines.toString()));
                        currentLines = new StringJoiner("\n");
                    }

                    currentTitle = normalizedTitle;
                } else {
                    currentLines.add(line);
                }
            }

            if (currentLines.length() > 0) {
                overviewMap.put(currentTitle, trimNewlines(currentLines.toString()));
            }
        }
    }

    final String cleanDescription = json.optString("clean_desc", null);
    final String impact = json.optString("impact", null);
    final String resolution = json.optString("resolution", null);
    final String workarounds = json.optString("workarounds", null);
    final String name = json.optString("name", null);

    entry.setSummary(firstNonEmpty(name, overviewMap.get("Overview"), cleanDescription));
    entry.addDescription(DescriptionParagraph.fromContent(longestNonEmpty(overviewMap.get("Description"), overviewMap.get(""), cleanDescription)));
    entry.setThreat(firstNonEmpty(overviewMap.get("Impact"), impact));
    entry.setRecommendations(firstNonEmpty(overviewMap.get("Solution"), resolution));
    entry.setWorkarounds(firstNonEmpty(overviewMap.get("Workarounds"), workarounds));

    entry.addAcknowledgements(Arrays.asList(json.optString("author", "").split("(,|\n\n)")));
    entry.addAcknowledgements(Arrays.asList(json.optString("thanks", "").split("(,|\n\n)")));
    entry.addAcknowledgements(Arrays.asList(overviewMap.getOrDefault("Acknowledgements", "").split("(,|\n\n)")));
}

{
    final JSONArray referencedCves = json.optJSONArray("cveids");
    if (referencedCves != null && !referencedCves.isEmpty()) {
        for (int i = 0; i < referencedCves.length(); i++) {
            final String id = referencedCves.optString(i, null);
            if (!StringUtils.hasText(id)) continue;

            if (id.contains(" ")) {
                entry.addReferencedVulnerabilities(VulnerabilityTypeStore.CVE, Arrays.asList(referencedCves.optString(i).split(" ?,? ")));
            } else {
                entry.addReferencedVulnerability(VulnerabilityTypeStore.CVE, referencedCves.optString(i));
            }
        }
    }

    if (entry.getReferencedVulnerabilities(VulnerabilityTypeStore.CVE).isEmpty()) {
        extractReferenceIdsFromStringIfPresent(entry.getSummary(), entry, VulnerabilityTypeStore.CVE);
        extractReferenceIdsFromStringIfPresent(entry.getTextDescription(), entry, VulnerabilityTypeStore.CVE);
        extractReferenceIdsFromStringIfPresent(entry.getThreat(), entry, VulnerabilityTypeStore.CVE);
        extractReferenceIdsFromStringIfPresent(entry.getRecommendations(), entry, VulnerabilityTypeStore.CVE);
        extractReferenceIdsFromStringIfPresent(entry.getWorkarounds(), entry, VulnerabilityTypeStore.CVE);
    }

    if (entry.getReferencedVulnerabilities(VulnerabilityTypeStore.CVE).isEmpty()) {
        LOG.debug("No referenced CVEs found for [{}], which makes it impossible for this advisory to be found during our enrichment.", entry.getId());
    }
}

{
    final String cvss2Base = json.optString("cvss_basevector", null);
    final String cvss2Environmental = json.optString("cvss_environmentalvector", null);

    final StringJoiner cvss2Joiner = new StringJoiner("/");
    if (StringUtils.hasText(cvss2Base) && !cvss2Base.equals("N/A") && !cvss2Base.contains("--")) {
        cvss2Joiner.add(cvss2Base);
    }
    if (StringUtils.hasText(cvss2Environmental) && !cvss2Environmental.equals("N/A") && !cvss2Environmental.contains("--")) {
        cvss2Joiner.add(cvss2Environmental);
    }

    if (!cvss2Joiner.toString().trim().isEmpty()) {
        final Cvss2 vector = new Cvss2(cvss2Joiner.toString(), new CvssSource(KnownCvssEntities.CERT_SEI, Cvss2.class));
        if (vector.isAnyBaseDefined() || vector.isAnyEnvironmentalDefined() || vector.isAnyTemporalDefined()) {
            entry.getCvssVectors().addCvssVector(vector);
        }
    }
}

return entry;
```

</details>


<p>The CERT-SEI provides multiple JSON files with one note/advisor per file. To parse the JSON files in the download
directory:</p>
<ol>
<li>iterate over all JSON files in the directory,</li>
<li>read and parse the JSON contents of each file,</li>
<li>use <code>CertSeiAdvisorEntry.fromDownloadJson</code> to map the fields to a <code>CertSeiAdvisorEntry</code> object.</li>
</ol>
<table>
<caption>Mapping of JSON content to <code>CertSeiAdvisorEntry</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">JSON</th>
<th style="text-align:left">CertSeiAdvisorEntry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>vuid</code>, <code>idnumber</code>, <code>id</code></td>
<td style="text-align:left"><code>id</code></td>
</tr>
<tr>
<td style="text-align:left"><code>datecreated</code>, <code>datefirstpublished</code>, <code>publicdate</code>, <code>dateupdated</code></td>
<td style="text-align:left"><code>createDate</code></td>
</tr>
<tr>
<td style="text-align:left"><code>dateupdated</code>, <code>datecreated</code>, <code>datefirstpublished</code>, <code>publicdate</code></td>
<td style="text-align:left"><code>updateDate</code></td>
</tr>
<tr>
<td style="text-align:left"><code>author</code> &amp; <code>thanks</code> &amp; <code>Acknowledgements</code></td>
<td style="text-align:left"><code>acknowledgements</code></td>
</tr>
<tr>
<td style="text-align:left"><code>public</code></td>
<td style="text-align:left"><code>references</code></td>
</tr>
<tr>
<td style="text-align:left"><code>keywords</code></td>
<td style="text-align:left"><code>keywords</code></td>
</tr>
<tr>
<td style="text-align:left"><code>cveids</code></td>
<td style="text-align:left"><code>referenceIds</code></td>
</tr>
<tr>
<td style="text-align:left"><code>cvss_basevector</code> &amp; <code>cvss_environmentalvector</code></td>
<td style="text-align:left"><code>cvss2</code></td>
</tr>
<tr>
<td style="text-align:left"><code>name</code>, <code>Overview</code>, <code>clean_description</code></td>
<td style="text-align:left"><code>summary</code></td>
</tr>
<tr>
<td style="text-align:left">longest(<code>Description</code>, <code>&quot;&quot;</code>, <code>clean_description</code>)</td>
<td style="text-align:left"><code>description</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Impact</code>, <code>impact</code></td>
<td style="text-align:left"><code>threat</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Solution</code>, <code>resolution</code></td>
<td style="text-align:left"><code>recommendation</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Workarounds</code>, <code>workarounds</code></td>
<td style="text-align:left"><code>workarounds</code></td>
</tr>
</tbody>
</table>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CERT-SEI](download.md#cert-sei) |


## CERT-FR Advisor

`certfr-advisors` / `certFrAdvisorIndex`

<details>

<summary>Source Code for <code>CertFrAdvisorIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final List<File> files = super.getAllFilesInSubDirectories(super.getRequiredDownload(CertFrDownload.class));

for (File file : files) {
    super.executor.submit(() -> {
        try {
            final CertFrAdvisorEntry parsedEntry;

            if (file.getName().endsWith(".txt")) {
                final List<String> contents = FileUtils.readLines(file, StandardCharsets.UTF_8);
                parsedEntry = CertFrAdvisorEntry.fromDownloadText(contents);
            } else if (file.getName().endsWith(".json")) {
                parsedEntry = CertFrAdvisorEntry.fromApiJson(file);
            } else {
                LOG.warn("Unsupported file format, skipping: {}", file.getAbsolutePath());
                return;
            }

            if (documents.containsKey(parsedEntry.getId())) {
                LOG.warn("Duplicate entry found, skipping: {}", parsedEntry.toJson());
            } else {
                documents.put(parsedEntry.getId(), parsedEntry.toDocument());
            }
        } catch (IOException e) {
            throw new RuntimeException("Unable to read file contents during indexing: " + file.getAbsolutePath(), e);
        } catch (Exception e) {
            throw new RuntimeException("Unable to parse file content during indexing: " + file.getAbsolutePath(), e);
        }
    });
}

super.executor.setSize(16);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>CertFrAdvisorEntry.fromDownloadText(List lines)</code></summary>

```java
final CertFrAdvisorEntry entry = new CertFrAdvisorEntry();

if (lines.isEmpty()) throw new IllegalArgumentException("No data found in file");


// find lines that should be processed
// ignore empty lines and lines without important content
lines = lines.stream()
        .map(l -> l.replace("\f", ""))
        .map(l -> l.replace("(cid:160)", ""))
        .map(String::trim)
        .filter(l -> Arrays.stream(CERT_FR_IGNORE_LINE_PATTERNS).noneMatch(ignoreLine -> ignoreLine.matcher(l).matches()))
        .collect(Collectors.toList());

// remove double newlines
final List<String> reducedMappedLines = new ArrayList<>();
boolean lastLineEmpty = false;
for (String line : lines) {
    if (lastLineEmpty && line.isEmpty()) {
        continue;
    }
    reducedMappedLines.add(line);
    lastLineEmpty = line.isEmpty();
}

final Map<String, StringJoiner> furtherContent = new HashMap<>();
int maxHeaderNumber = 0;

// has to be a fori loop, as lines are being skipped and lookaheads are performed inside the loop
for (int i = 0; i < reducedMappedLines.size(); i++) {
    String line = reducedMappedLines.get(i);

    // depending on the archive and year, the format can be different, therefore the different sections of the
    // document need to be found first before they can be processed
    if (line.startsWith("Objet:")) { // topic
        entry.setSummary(line.replace("Objet: ", ""));

    } else if (line.equals("Gestion du document")) { // document data in table
        final ArrayList<String> tableContent = new ArrayList<>();
        i++;
        line = reducedMappedLines.get(i);

        // continue collecting table entries until end of table is reached
        while (!(line.contains("Tableau") || line.contains("TAB")) || line.contains("Electric")) {
            tableContent.add(line);
            i++;
            if (i >= reducedMappedLines.size()) break;
            line = reducedMappedLines.get(i);
        }

        certFrExtractContentsFromTable(entry, tableContent);

    } else if (line.equals("Gestion détaillée du document")) {
        // document history at the bottom of the document, the following lines do not contain important content any more
        break;
    } else if (certFrIsHeader(line, maxHeaderNumber, reducedMappedLines, i)) {
        // if a header is found, collect the following lines until either another header is found or the end of the
        // document (Gestion détaillée du document) is found.

        final String header = certFrNormalizeHeaderText(line);
        maxHeaderNumber = certFrExtractHeaderNumber(line, maxHeaderNumber);

        final List<String> contents = new ArrayList<>();

        // skip to next line and continue adding lines until header or end of document is reached
        i++;
        if (i >= lines.size()) break;
        line = reducedMappedLines.get(i);
        while (!certFrIsHeader(line, maxHeaderNumber, reducedMappedLines, i) && !line.equals("Gestion détaillée du document")) {
            contents.add(line);
            i++;
            if (i >= reducedMappedLines.size()) break;
            line = reducedMappedLines.get(i);
        }

        // add the paragraph to the further details
        furtherContent.computeIfAbsent(header, h -> new StringJoiner("\n")).add(String.join("\n", contents));
        i--;
    }
}


final Map<String, String> trimmedFurtherContent = furtherContent.entrySet()
        .stream()
        .collect(Collectors.toMap(Map.Entry::getKey, e -> trimNewlines(e.getValue().toString().trim()).trim()));

// the further content map may contain a field named "Summary". This field may become very long however (see
// CERTFR-2021-ALE-022), which is why it cannot be used for the summary attribute of the AdvisorEntry, which is
// reserved for a short description.

final StringJoiner recommendationBuilder = new StringJoiner("\n\n");
if (trimmedFurtherContent.containsKey("Recommendations")) {
    recommendationBuilder.add(trimmedFurtherContent.remove("Recommendations"));
}
if (trimmedFurtherContent.containsKey("Solution")) {
    recommendationBuilder.add(trimmedFurtherContent.remove("Solution"));
}
if (recommendationBuilder.length() > 0) {
    entry.setRecommendations(recommendationBuilder.toString());
}

entry.setThreat(trimmedFurtherContent.remove("Risk"));

entry.setWorkarounds(trimmedFurtherContent.remove("Temporary bypass"));

if (trimmedFurtherContent.containsKey("Documentation")) {
    final String[] documentationLines = trimmedFurtherContent.remove("Documentation").split("\n");

    for (int i = 0; i < documentationLines.length - 1; i += 2) {
        final Reference ref = Reference.fromTitleAndUrl(documentationLines[i], documentationLines[i + 1]);
        entry.getReferences().add(ref);
    }
    entry.clearDescription();
} else {
    final String sourcesDescription = entry.getTextDescription();
    entry.setDescription(DescriptionParagraph.fromTitleAndContent("Sources", sourcesDescription));
}

if (trimmedFurtherContent.containsKey("Description")) {
    entry.addDescription(DescriptionParagraph.fromTitleAndContent("Description", trimmedFurtherContent.remove("Description")));
}

// merge all further contents into a single description
for (String header : trimmedFurtherContent.keySet()) {
    entry.addDescription(DescriptionParagraph.fromTitleAndContent(header, trimmedFurtherContent.get(header)));
}

final String inputAsOneLine = String.join("", reducedMappedLines);
extractReferenceIdsFromStringIfPresent(inputAsOneLine, entry, AdvisoryTypeStore.CERT_FR);
extractReferenceIdsFromStringIfPresent(inputAsOneLine, entry, VulnerabilityTypeStore.CVE);

if (!StringUtils.hasText(entry.getId())) {
    LOG.error("Unable to detect CERT-FR identifier in document: {}", entry.toJson());
} else {
    entry.removeReferencedSecurityAdvisory(AdvisoryTypeStore.CERT_FR, entry.getId());
}

if (entry.getCreateDate() == null) {
    LOG.warn("Missing create date on {}", entry.getId());
}
if (entry.getUpdateDate() == null) {
    LOG.warn("Missing update date on {}", entry.getId());
}

return entry;
```

</details>


<details>

<summary>Source Code for <code>CertFrAdvisorEntry.fromApiJson(JSONObject documentJson)</code></summary>

```java
final CertFrAdvisorEntry entry = new CertFrAdvisorEntry();

String id = documentJson.optString("reference");
if (id.matches("\\d+.+")) {
    id = id.replaceAll("\\d+(.+)", "$1");
}
id = id.toUpperCase(Locale.ENGLISH).trim();
entry.setId(id);
entry.setSummary(documentJson.optString("title", null));

final String contentField = documentJson.optString("content", null);
if (StringUtils.hasText(contentField)) {
    final List<String> lines = Arrays.asList(contentField.split("\n"));
    final Map<String, StringJoiner> furtherContent = new HashMap<>();

    for (int i = 0; i < lines.size(); i++) {
        String line = lines.get(i);
        if (certFrIsHeader(line, 1, lines, i)) {
            // if a header is found, collect the following lines until either another header is found or the end of the
            // document (Gestion détaillée du document) is found.

            final String header = certFrNormalizeHeaderText(line);
            final List<String> contents = new ArrayList<>();

            // skip to next line and continue adding lines until header or end of document is reached
            i++;
            if (i >= lines.size()) break;
            line = lines.get(i);
            while (!certFrIsHeader(line, 1, lines, i) && !line.equals("Gestion détaillée du document")) {
                contents.add(line);
                i++;
                if (i >= lines.size()) break;
                line = lines.get(i);
            }

            // add the paragraph to the further details
            furtherContent.computeIfAbsent(header, h -> new StringJoiner("\n")).add(String.join("\n", contents));
            i--;
        }
    }

    final Map<String, String> trimmedFurtherContent = furtherContent.entrySet()
            .stream()
            .collect(Collectors.toMap(Map.Entry::getKey, e -> trimNewlines(e.getValue().toString().trim()).trim()));

    if (trimmedFurtherContent.containsKey("Temporary bypass")) {
        entry.setWorkarounds(trimmedFurtherContent.remove("Temporary bypass"));
    }
    if (trimmedFurtherContent.containsKey("Solution") || trimmedFurtherContent.containsKey("Recommendations")) {
        final StringJoiner recommendationsBuilder = new StringJoiner("\n\n");
        if (trimmedFurtherContent.containsKey("Solution")) {
            recommendationsBuilder.add(trimmedFurtherContent.remove("Solution"));
        }
        if (trimmedFurtherContent.containsKey("Recommendations")) {
            recommendationsBuilder.add(trimmedFurtherContent.remove("Recommendations"));
        }
        entry.setRecommendations(recommendationsBuilder.toString());
    }
    if (trimmedFurtherContent.containsKey("Documentation")) {
        final String[] documentationLines = trimmedFurtherContent.remove("Documentation").split("\n");

        for (int i = 0; i < documentationLines.length - 1; i += 2) {
            final Reference ref = Reference.fromTitleAndUrl(documentationLines[i], documentationLines[i + 1]);
            entry.getReferences().add(ref);
        }
    }

    for (String header : trimmedFurtherContent.keySet()) {
        entry.addDescription(DescriptionParagraph.fromTitleAndContent(header, trimmedFurtherContent.get(header)));
    }
}
if (StringUtils.hasText(documentJson.optString("summary", null))) {
    entry.addDescription(DescriptionParagraph.fromTitleAndContent("Summary", documentJson.getString("summary").replace("\n\n\u00a0", "").replaceAll("\n+$", "")));
}

final JSONArray revisions = documentJson.optJSONArray("revisions");
if (revisions != null && !revisions.isEmpty()) {
    final JSONObject firstRevision = revisions.getJSONObject(0);
    entry.setCreateDate(TimeUtils.tryParse(firstRevision.optString("revision_date", null)));

    final JSONObject lastRevision = revisions.getJSONObject(revisions.length() - 1);
    entry.setUpdateDate(TimeUtils.tryParse(lastRevision.optString("revision_date", null)));
}

final JSONArray risks = documentJson.optJSONArray("risks");
if (risks != null && !risks.isEmpty()) {
    final StringJoiner riskJoiner = new StringJoiner("\n");
    for (int i = 0; i < risks.length(); i++) {
        final String risk = risks.getJSONObject(i).optString("description", null);
        if (risk != null) riskJoiner.add(risk);
    }
    entry.setThreat(riskJoiner.toString());
}

final List<String> affectedSystems = new ArrayList<>();
final JSONArray affectedSystemsArray = documentJson.optJSONArray("affected_systems");
if (affectedSystemsArray != null) {
    for (int i = 0; i < affectedSystemsArray.length(); i++) {
        final JSONObject affectedSystem = affectedSystemsArray.getJSONObject(i);
        final String affectedSystemDescription = affectedSystem.getString("description");
        if (affectedSystem.optJSONObject("product") != null) {
            final String productName = affectedSystem.getJSONObject("product").getString("name");
            if (StringUtils.hasText(productName) && !productName.equals("N/A")) {
                affectedSystems.add(productName + ": " + affectedSystemDescription);
            } else {
                affectedSystems.add(affectedSystemDescription);
            }
        } else {
            affectedSystems.add(affectedSystemDescription);
        }
    }
}
if (documentJson.optString("affected_systems_content", null) != null) {
    affectedSystems.add(documentJson.getString("affected_systems_content"));
}
if (!affectedSystems.isEmpty()) {
    entry.addDescription("Affected systems", affectedSystems.stream().sorted().collect(Collectors.joining("\n")));
}

final JSONArray links = documentJson.optJSONArray("links");
if (links != null && !links.isEmpty()) {
    for (int i = 0; i < links.length(); i++) {
        final JSONObject link = links.optJSONObject(i);
        if (link != null) {
            entry.addReference(Reference.fromTitleAndUrl(link.getString("title"), link.getString("url")));
        }
    }
}

final JSONArray cves = documentJson.optJSONArray("cves");
if (cves != null && !cves.isEmpty()) {
    for (int i = 0; i < cves.length(); i++) {
        final JSONObject cve = cves.optJSONObject(i);
        if (cve != null) {
            entry.addReferencedVulnerability(VulnerabilityTypeStore.CVE, cve.getString("name"));
        }
    }
}

final JSONArray vendorAdvisories = documentJson.optJSONArray("vendor_advisories"); // add as references
if (vendorAdvisories != null && !vendorAdvisories.isEmpty()) {
    for (int i = 0; i < vendorAdvisories.length(); i++) {
        final JSONObject vendorAdvisory = vendorAdvisories.optJSONObject(i);
        if (vendorAdvisory != null) {
            final String url = vendorAdvisory.optString("url", null);
            if (url != null) {
                final Reference reference = Reference.fromTitleAndUrl(vendorAdvisory.optString("title", null), url);
                if (vendorAdvisory.optString("published_at", null) != null) {
                    reference.addTag(vendorAdvisory.getString("published_at"));
                }
                entry.addReference(reference);
            }
        }
    }
}

AdvisoryEntry.extractReferenceIdsFromStringIfPresent(entry.getTextDescription(), entry, AdvisoryTypeStore.CERT_FR);
AdvisoryEntry.extractReferenceIdsFromStringIfPresent(entry.getTextDescription(), entry, VulnerabilityTypeStore.CVE);
AdvisoryEntry.extractReferenceIdsFromStringIfPresent(entry.getReferences().stream().map(Reference::toString).collect(Collectors.joining(", ")), entry, AdvisoryTypeStore.CERT_FR);

return entry;
```

</details>


<h3>TXT</h3>
<p>The TXT files provided by the CERT-FR are mere transcriptions of PDF files and are highly unstructured, lacking proper
segmentation, including header and footer on each page and PDF-specific formatting symbols. Each document
contains a table in the header that provides some general information, such as the document ID and dates. The table rows
appear in random order from document to document.</p>
<p>After the header, there are multiple titles followed by text content. These headers are not normalized, with over 1800 unique
headers, where sometimes there are variations of headers and some appear only once. After detecting the paragraphs, they
are collected with their respective header and text content. Some titles are normalized to an English identifier for
consistency.</p>
<ol>
<li>Iterate over all TXT files in the directory</li>
<li>Read the contents of each file</li>
<li>Use <code>CertFrAdvisorEntry.fromDownloadText</code> to map the information to a <code>CertFrAdvisorEntry</code> object</li>
</ol>
<table>
<caption>Mapping of TXT content to <code>CertFrAdvisorEntry</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">TXT</th>
<th style="text-align:left">CertSeiAdvisorEntry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table &gt; first line that matches CERT-FR identifier format</td>
<td style="text-align:left"><code>id</code></td>
</tr>
<tr>
<td style="text-align:left">table &gt; <code>Date de la première version</code> OR first line that matches date</td>
<td style="text-align:left"><code>createDate</code></td>
</tr>
<tr>
<td style="text-align:left">table &gt; <code>Date de la dernière version</code> OR second line that matches date</td>
<td style="text-align:left"><code>updateDate</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Documentation</code> &gt; every second line contains an URL with the line above being the title</td>
<td style="text-align:left"><code>references</code></td>
</tr>
<tr>
<td style="text-align:left">CERT-FR and CVEs in the entire text content</td>
<td style="text-align:left"><code>referenceIds</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Summar</code>, header &gt; <code>Object:</code></td>
<td style="text-align:left"><code>summary</code></td>
</tr>
<tr>
<td style="text-align:left">(if <code>Documentation</code> is absent: table &gt; <code>sources</code>) &amp; <code>Description</code> &amp; all other further unstructured paragraphs that have not been used otherwise</td>
<td style="text-align:left"><code>description</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Risk</code></td>
<td style="text-align:left"><code>threat</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Recommendations</code> &amp; <code>Solution</code></td>
<td style="text-align:left"><code>recommendation</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Temporary bypass</code></td>
<td style="text-align:left"><code>workarounds</code></td>
</tr>
</tbody>
</table>
<p>Valid table headers are:</p>
<ul>
<li><code>Systèmes affectés</code>, <code>Affected systems</code></li>
<li><code>Résumé</code>, <code>Summary</code></li>
<li><code>Risque\\(s\\)</code>, <code>Risques</code>, <code>Risque</code>, <code>Risk</code></li>
<li><code>Solution</code>, <code>Solutions</code>, <code>Solution</code></li>
<li><code>Recommandations</code>, <code>Recommendations</code></li>
<li><code>Documentation</code>, <code>Documentations</code>, <code>Documentation</code></li>
<li><code>Contournement</code> <code>provisoire</code>, <code>Temporary bypass</code></li>
<li><code>Description</code>, <code>Description</code></li>
<li><code>Rappel des avis émis</code>, <code>Rappel des avis et des mises à jour émis</code>, <code>Rappel des avis et mises à jour émis</code>,
<code>Reminder of notices issued</code></li>
<li><code>^\\d{1,2} .+</code></li>
</ul>
<h3>JSON</h3>
<p>Luckily, there is an alternative: Our downloader-preprocessing also builds JSON files from the API, which are much more structured.
The JSON files contain the same information as the TXT files, but in a structured format.
The fields used are the same as in the TXT files.</p>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CERT-FR](download.md#cert-fr) |


## CERT-EU Advisor

`certeu-advisors` / `certEuAdvisorIndex`

<details>

<summary>Source Code for <code>CertEuAdvisorIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

final List<File> files = super.getAllFilesInSubDirectories(new File(super.getRequiredDownload(CertEuDownload.class), "publications"));
LOG.info("Found [{}] CERT-EU advisory files", files.size());

for (File file : files) {
    try {
        final List<String> contents = FileUtils.readLines(file, StandardCharsets.UTF_8);
        final JSONObject jsonContent = new JSONObject(String.join("", contents));

        final CertEuAdvisorEntry parsedEntry = CertEuAdvisorEntry.fromDownloadJson(jsonContent);

        documents.put(parsedEntry.getId(), parsedEntry.toDocument());
    } catch (IOException e) {
        throw new RuntimeException("Unable to read file contents during indexing ", e);
    }
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>CertEuAdvisorEntry.fromDownloadJson(JSONObject json)</code></summary>

```java
final CertEuAdvisorEntry certEuAdvisorEntry = new CertEuAdvisorEntry();

certEuAdvisorEntry.setId("CERT-EU-" + json.getString("serial_number"));
certEuAdvisorEntry.setSummary(json.optString("title", null));

if (json.has("description") && json.get("description") != JSONObject.NULL) {
    certEuAdvisorEntry.addDescription(DescriptionParagraph.fromTitleAndContent("description", json.getString("description")));
    extractReferenceIdsFromStringIfPresent(json.getString("description"), certEuAdvisorEntry, VulnerabilityTypeStore.CVE);
}
if (json.has("content_markdown") && json.get("content_markdown") != JSONObject.NULL) {
    extractMarkdownChaptersContent(json.getString("content_markdown"), certEuAdvisorEntry);
    extractReferenceIdsFromStringIfPresent(json.getString("content_markdown"), certEuAdvisorEntry, VulnerabilityTypeStore.CVE);
}
if (json.has("content_html") && json.get("content_html") != JSONObject.NULL) {
    certEuAdvisorEntry.addDescription(DescriptionParagraph.fromTitleAndContent("content_html", json.getString("content_html")));
    extractReferenceIdsFromStringIfPresent(json.getString("content_html"), certEuAdvisorEntry, VulnerabilityTypeStore.CVE);
}

try {
    // 20-06-2017 13:40:00
    final long createDate = new SimpleDateFormat("dd-MM-yyyy HH:mm:ss").parse(json.getString("publish_date")).getTime();
    certEuAdvisorEntry.setCreateDate(createDate);
    certEuAdvisorEntry.setUpdateDate(createDate);
} catch (ParseException e) {
    LOG.error("Unable to parse date from JSON: {}", json.getString("publish_date"));
}

return certEuAdvisorEntry;
```

</details>


<p>The CERT-EU provides multiple JSON files with one note/advisor per file.
This process is mostly the same as for all other security advisories.</p>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CERT-EU](download.md#cert-eu) |


## KEV

`kev` / `kevIndex`

<details>

<summary>Source Code for <code>KevIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final Collection<File> files = super.getAllFilesRecursively(super.getRequiredDownload(CisaKevDownload.class));

for (File file : files) {
    if (!file.getName().endsWith(".json")) {
        continue;
    }
    log.info("Processing file: {}", file.getName());

    try {
        final String contents = FileUtils.readFileToString(file, StandardCharsets.UTF_8);
        final JSONObject parsedContents = new JSONObject(contents);
        final JSONArray kevEntries = parsedContents.getJSONArray("vulnerabilities");
        final List<Document> kevEntriesDocs = fromJson(kevEntries);


        for (Document document : kevEntriesDocs) {
            String uldid = document.get("vulnerability");
            if (documents.containsKey(uldid)) {
                log.warn("Duplicate entry found, skipping: {}", document);
            } else {
                documents.put(uldid, document);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Failed to read file: " + file.getAbsolutePath(), e);
    } catch (JSONException e) {
        throw new RuntimeException("Failed to parse JSON: " + file.getAbsolutePath(), e);
    }
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>KevData.fromCisaKev(JSONObject json)</code></summary>

```java
final Map<String, Object> map = json.toMap();
final KevData kevData = new KevData();
kevData.setVulnerability((String) map.getOrDefault("cveID", null));
kevData.setVendor((String) map.getOrDefault("vendorProject", null));
kevData.setProduct((String) map.getOrDefault("product", null));
kevData.setSummary((String) map.getOrDefault("vulnerabilityName", null));
kevData.setRecommendation((String) map.getOrDefault("requiredAction", null));
kevData.setNotes((String) map.getOrDefault("notes", null));
kevData.setDescription((String) map.getOrDefault("shortDescription", null));
// publication date is not used as exploitation date, since CISA does not always publish KEV entries upon known exploitation,
// but also sometimes (often) retroactively adds 20-year-old vulnerabilities as exploited vulnerabilities.
// https://www.linkedin.com/pulse/epss-gaps-cisa-kev-real-world-examples-stack-aware-url2e/
kevData.setPublishDate(TimeUtils.tryParse((String) map.getOrDefault("dateAdded", null)));
kevData.setDueDate(TimeUtils.tryParse((String) map.getOrDefault("dueDate", null)));

String ransomwareKnown = (String) map.getOrDefault("knownRansomwareCampaignUse", null);
if (ransomwareKnown != null) {
    switch (ransomwareKnown) {
        case "Known":
            kevData.setRansomwareState(RansomwareState.KNOWN);
            break;
        case "Unknown":
        default:
            kevData.setRansomwareState(RansomwareState.UNKNOWN);
    }
}
return kevData;
```

</details>


<p>This index processes Known Exploited Vulnerabilities (KEV) from CISA, which are provided in JSON format.
Each file contains multiple entries describing vulnerabilities, which are mapped to a document format using Apache Lucene for indexing.
It is used to add priority information to existing vulnerabilities.</p>

<p>JSON Structure:</p>
<pre lang="json"><code class="lang-json">{
  "vulnerabilities": [
    {
      "cveID": "CVE-2022-12345",
      "vendorProject": "VendorX",
      "product": "ProductY",
      "vulnerabilityName": "Example Vulnerability",
      "requiredAction": "Apply patch",
      "notes": "None",
      "shortDescription": "This is a sample description.",
      "dateAdded": "2022-07-14",
      "dueDate": "2022-09-14",
      "knownRansomwareCampaignUse": "Known"
    }
  ]
}</code></pre>

<p>The KEV data files are processed as follows:</p>
<ol>
<li>All JSON files are recursively read from the specified directory.</li>
<li>Each JSON file is parsed, and all entries in the "vulnerabilities" array are processed.</li>
<li>Each entry is converted into a <code>Document</code> object for indexing.</li>
</ol>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CISA KEV](download.md#cisa-kev) |


## EPSS

`epss` / `epssIndex`

<details>

<summary>Source Code for <code>EpssIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final Collection<File> files = super.getAllFilesRecursively(super.getRequiredDownload(EpssDownload.class));
for (File file : files) {
    if (!file.getName().endsWith(".csv")) {
        continue;
    }

    log.info("Processing file: {}", file.getName());

    try {
        String contents = FileUtils.readFileToString(file, StandardCharsets.UTF_8);
        contents = contents.substring(contents.indexOf("\n") + 1);
        contents = contents.substring(contents.indexOf("\n") + 1);

        for (String line : contents.split("\n")) {
            String[] fields = line.split(",");
            EpssData epssData = new EpssData(fields[0], Float.parseFloat(fields[1]), Float.parseFloat(fields[2]));
            documents.put(fields[0], epssData.toDocument());
        }

    } catch (IOException e) {
        throw new RuntimeException("Failed to read file: " + file.getAbsolutePath(), e);
    }
}

return documents;
```

</details>


<p>This index processes EPSS (Exploit Prediction Scoring System) data, which is provided in CSV format.
Each file contains various entries that are mapped to an internal document format using Apache Lucene for indexing and later retrieval.
This index is primarily used for ranking vulnerabilities by the likelihood of exploitation based on historical data.
It is used to add priority information to existing vulnerabilities.</p>

<p>CSV Structure:</p>
<pre lang="csv"><code class="lang-csv">cve_id,epss_score,percentile
CVE-2022-12345,0.97,99.7
CVE-2021-54321,0.45,50.2
...</code></pre>

<p>The EPSS data files are processed as follows:</p>
<ol>
<li>Each file is parsed, and non-CSV files are skipped.</li>
<li>The first two lines of the CSV file (header and metadata) are discarded.</li>
<li>Each line is read, split into individual fields, and stored as a document object.</li>
</ol>

<table>
<caption>Mapping of CSV content to <code>EpssData</code> fields</caption>
<thead>
<tr>
<th style="text-align:left">CSV Field</th>
<th style="text-align:left">Mapped Document Field</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>cve_id</code></td>
<td style="text-align:left"><code>cveId</code></td>
</tr>
<tr>
<td style="text-align:left"><code>epss_score</code></td>
<td style="text-align:left"><code>epssScore</code></td>
</tr>
<tr>
<td style="text-align:left"><code>percentile</code></td>
<td style="text-align:left"><code>percentile</code></td>
</tr>
</tbody>
</table>

| Dependency Type | Depends on |
| --- | --- |
| **Download** | [EPSS](download.md#epss) |


## EOL

`eol` / `eolIndex`

<details>

<summary>Source Code for <code>EolIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final Collection<File> files = super.getAllFilesRecursively(super.getRequiredDownload(EolDownload.class));
log.info("Found [{}] files in repository", files.size());

final EolDataParser parser = super.configureDataParser(new EolDataParser());

for (File file : files) {
    if (!file.getName().endsWith(".json")) {
        continue;
    }

    final String product = file.getName().replace(".json", "");

    try {
        final String contents = FileUtils.readFileToString(file, StandardCharsets.UTF_8);
        final JSONArray parsedContents = new JSONArray(contents);
        final EolLifecycle productInfo = parser.lifecycleFromJson(product, parsedContents);

        for (Document document : productInfo.toDocuments()) {
            String uldid = product + "-" + document.get("cycle");
            if (documents.containsKey(uldid)) {
                uldid = uldid + "-" + document.get("link");
                if (documents.containsKey(uldid)) {
                    log.warn("Duplicate entry found, skipping: {}", document);
                }
            } else {
                documents.put(uldid, document);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Failed to read file: " + file.getAbsolutePath(), e);
    } catch (Exception e) {
        super.mirrorResult.createObserver().onFailure(log, new RuntimeException("Failed to parse EOL Cycle information for product file " + FileUtils.canonicalOrAbsolutePath(file), e));
    }
}

if (!files.isEmpty()) {
    log.info("Average of [{}] cycles per product", documents.size() / files.size());
}

return documents;
```

</details>


<details>

<summary>Source Code for <code>EolDataParser.lifecycleFromJson(String product, JSONArray json)</code></summary>

```java
final EolLifecycle productInfo = new EolLifecycle(product);

for (int i = 0; i < json.length(); i++) {
    try {
        final EolCycle cycle = EolCycle.fromJson(json.getJSONObject(i));
        cycle.setProduct(product);
        productInfo.addCycle(cycle);
    } catch (RuntimeException e) {
        super.observer.onFailure(log, new RuntimeException("Failed to parse EOL for [" + product + "] from JSON: " + json.optJSONObject(i) + "\n" + e.getMessage(), e));
    }
}

return productInfo;
```

</details>


<details>

<summary>Source Code for <code>EolCycle.fromJson(JSONObject json)</code></summary>

```java
return new EolCycle(
        parseJson(json, "product"),
        parseJson(json, "cycle"),
        parseJson(json, "releaseDate"),
        parseJson(json, "eol"),
        parseJson(json, "latest"),
        parseJson(json, "link"),
        parseJson(json, "lts"),
        parseJson(json, "support"),
        parseJson(json, "discontinued"),
        parseJson(json, "latestReleaseDate"),
        firstNonNull(parseJson(json, "supportedPhpVersions"), parseJson(json, "supportedPHPVersions")),
        parseJson(json, "latestJdkVersion"),
        parseJson(json, "extendedSupport"),
        parseJson(json, "upgradeVersion"),
        parseJson(json, "releaseLabel"),
        parseJson(json, "supportedKubernetesVersions"),
        parseJson(json, "minRubyVersion"),
        parseJson(json, "codename"),
        parseJson(json, "technicalGuidance"),
        parseJson(json, "supportedJavaVersions"),
        parseJson(json, "minJavaVersion"));
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | [EOL](download.md#eol) |


## CWE

`cwe` / `cweIndex`

<details>

<summary>Source Code for <code>CweIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();
final Map<String, List<String>> parentOfs = new HashMap<>();


final Collection<File> files = super.getAllFilesRecursively(super.getRequiredDownload(CweDownload.class));
for (File file : files) {
    if (!file.getName().matches("cwe.+\\.xml")) {
        continue;
    }

    log.info("Processing file: {}", file.getName());

    final List<CweEntry> cweEntries = parseCwesFromFile(file, parentOfs);
    final AtomicInteger n = new AtomicInteger();
    cweEntries.forEach(e -> n.addAndGet(e.getRelatedCapecs().size()));
    log.info("Found {} CWEs", cweEntries.size());
    log.info("Found {} with CAPECs", cweEntries.stream().filter(e -> !e.getRelatedCapecs().isEmpty()).count());
    log.info("Found {} total CAPECs", n);

    for (CweEntry cweEntry : cweEntries) {
        documents.put(cweEntry.getId(), cweEntry.toDocument());
    }

}

// add ParentOf relation to each document
addParentsOfRelations(documents, parentOfs);
return documents;
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CWE](download.md#cwe) |


## CAPEC

`capec-database` / `capecIndex`

<details>

<summary>Source Code for <code>CapecIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();
final Collection<File> files = new ArrayList<>(super.getAllFilesRecursively(super.getRequiredDownload(CapecDownload.class)));
final Map<String, List<String>> parentOfs = new HashMap<>();

for (File file : files) {
    if (!file.getName().endsWith(".xml")) {
        continue;
    }
    try {
        final org.w3c.dom.Document document = parseXmlDocument(FileUtils.readFileToString(file, StandardCharsets.UTF_8));

        final NodeList allReferences = document.getElementsByTagName("External_Reference");
        Map<String, Map <String, String>> referencesMap = new HashMap<>();
        for (int i = 0; i < allReferences.getLength(); i++) {
            final Element element = (Element) allReferences.item(i);
            String referenceId = element.getAttribute("Reference_ID");
            NodeList childNodes = element.getChildNodes();
            HashMap<String, String> entry = new HashMap<>();
            for (int j = 0; j < childNodes.getLength(); j++) {
                if (childNodes.item(j).getNodeType() != Node.ELEMENT_NODE)
                    continue;
                entry.put(childNodes.item(j).getNodeName().toLowerCase(), childNodes.item(j).getTextContent());

            }
            referencesMap.put(referenceId, entry);
        }

        final List<Element> elements = CapecEntry.findRelevantElementsFromDownloadXml(document);
        for (Element element : elements) {
            final CapecEntry parsedEntry = CapecEntry.fromDownloadXml(element, referencesMap, parentOfs);
            documents.put(parsedEntry.getId(), parsedEntry.toDocument());
        }
    } catch (IOException e) {
        throw new RuntimeException("Unable to read file contents during indexing: " + file.getAbsolutePath(), e);
    } catch (Exception e) {
        throw new RuntimeException("Unable to parse file content during indexing: " + file.getAbsolutePath(), e);
    }
}

// add ParentOf relation to each document
addParentsOfRelations(documents, parentOfs);

return documents;
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | [CAPEC](download.md#capec) |


## _NVD Vulnerability (deprecated)_

`nvd-cve` / `nvdLegacyVulnerabilityIndex`

<details>

<summary>Source Code for <code>NvdVulnerabilityIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new ConcurrentHashMap<>();

final File[] files = super.getRequiredDownload(NvdDownload.class).listFiles(f -> f.isFile() && f.getName().startsWith("nvd-") && f.getName().endsWith(".json"));

if (files == null) {
    throw new RuntimeException("Unable to list files in " + super.getRequiredDownload(NvdDownload.class));
}

final NvdVulnerabilityParser vulnerabilityParser = super.configureDataParser(new NvdVulnerabilityParser());

for (File file : files) {
    super.executor.submit(() -> documents.putAll(processFile(file, vulnerabilityParser)));
}

super.executor.setSize(6);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for indexing to complete.", e);
}

return documents;
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | _NVD_ |


## _CPE Dictionary (deprecated)_

`cpe-dict` / `cpeDictionaryIndex`

<details>

<summary>Source Code for <code>CpeDictionaryIndex.createIndexDocuments()</code></summary>

```java
final Map<String, Document> documents = new HashMap<>();

final File downloadsDirectory = super.getRequiredDownload(CpeDictionaryDownload.class);

final File cpeDictionary = new File(downloadsDirectory, "cpe-dict.xml");
if (!cpeDictionary.isFile()) {
    throw new RuntimeException("Could not find cpe-dict.xml in " + downloadsDirectory.getAbsolutePath());
}

final File cpeMatch = new File(downloadsDirectory, "cpe-match.json");
if (!cpeMatch.isFile()) {
    throw new RuntimeException("Could not find cpe-match.json in " + downloadsDirectory.getAbsolutePath());
}

parseCpeMatch(cpeMatch, documents);
// to prevent all the CPE information being stored in memory, write the existing documents into the index
super.writeIndexDocuments(documents);

parseCpeDict(cpeDictionary, documents);

return documents;
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Download** | _CPE Dictionary_ |


## _CPE Dictionary Vendor Product (deprecated)_

`cpe-dict-vp` / `cpeDictionaryVendorProductIndex`

<details>

<summary>Source Code for <code>CpeDictionaryVendorProductIndex.createIndexDocuments()</code></summary>

```java
final CpeDictionaryIndexQuery indexQuery = new CpeDictionaryIndexQuery(super.getRequiredIndex(CpeDictionaryIndex.class));

final Map<String, Document> documents = new HashMap<>();

LOG.info("Parsing all [Vendor Products] / [Product Vendors] pairs");
final Map<String, Set<String>> vendorProductsMap = new HashMap<>();
final Map<String, Set<String>> productVendorsMap = new HashMap<>();

indexQuery.getIndex().findAndProcessAllDocuments(document -> {
    vendorProductsMap.computeIfAbsent(document.get("vendor"), k -> new HashSet<>()).add(document.get("product"));
    productVendorsMap.computeIfAbsent(document.get("product"), k -> new HashSet<>()).add(document.get("vendor"));
});
LOG.info("Extracted [{} Vendor Products] and [{} Product Vendors]", vendorProductsMap.size(), productVendorsMap.size());

LOG.info("Building Vendor Products map");
for (Map.Entry<String, Set<String>> vendorProducts : vendorProductsMap.entrySet()) {
    final StringJoiner productJoiner = new StringJoiner(", ");

    for (String product : vendorProducts.getValue()) {
        productJoiner.add(product);
    }

    final Document document = new Document();

    document.add(new StringField("type", "vp", Field.Store.YES));
    document.add(new TextField("vendor", vendorProducts.getKey(), Field.Store.YES));
    document.add(new TextField("product", productJoiner.toString(), Field.Store.YES));

    documents.put("vp" + vendorProducts.getKey(), document);
}

LOG.info("Building Product Vendors map");
for (Map.Entry<String, Set<String>> productVendors : productVendorsMap.entrySet()) {
    final StringJoiner vendorJoiner = new StringJoiner(", ");

    for (String vendor : productVendors.getValue()) {
        vendorJoiner.add(vendor);
    }

    final Document document = new Document();

    document.add(new StringField("type", "pv", Field.Store.YES));
    document.add(new TextField("product", productVendors.getKey(), Field.Store.YES));
    document.add(new TextField("vendor", vendorJoiner.toString(), Field.Store.YES));

    documents.put("pv" + productVendors.getKey(), document);
}

return documents;
```

</details>


| Dependency Type | Depends on |
| --- | --- |
| **Index** | _CPE Dictionary_ |


# Global Configurations

## CPE Corrections

In the case of mirrors containing invalid CPE's, many of the Indexers would fail to parse the CPE's. Inorder to prevent
this, it is possible to define certain 'CPE Corrections'. Any Indexing Process would use these
mappings instead of the faulty ones provided by the mirror

Example:

<!-- @formatter:off -->
```xml
<configuration>
    <msrcProductIndex/>
    <msrcAdvisorIndex/>

    <cpeCorrectionConfigs>
        <correctionConfig>
            <malformedCpe>cpe:2.3:sainwp:onestore_sites:*:*:*:*:*:*:wordpress:*:*</malformedCpe>
            <replacementCpe>cpe:2.3:a:sainwp:onestore_sites:*:*:*:*:*:wordpress:*:*</replacementCpe>
        </correctionConfig>
        <correctionConfig>
            <malformedCpe>cpe:2.3:sainwp:onestore_blogs:*:*:*:*:*:*:wordpress:*:*</malformedCpe>
            <replacementCpe>cpe:2.3:a:sainwp:onestore_blogs:*:*:*:*:*:wordpress:*:*</replacementCpe>
        </correctionConfig>
    </cpeCorrectionConfigs>
</configuration>
```
<!-- @formatter:on -->
