> [Documentation](../../README.md) >
> [Vulnerability Management](../vulnerability-management.md) >
> [Vulnerability Data Mirror](vulnerability-data-mirror.md) >
> Downloaders

# Downloaders

> [Overview](#overview) -
> [POM Configuration](#pom-configuration) -
> [General Downloading Workflow](#general-downloading-workflow) -
> [Downloader Details](#downloader-details)

## Overview

Downloaders are responsible for retrieving data from a remote source and storing it in a local directory.
The implementation of each downloader varies depending on the format of the data provided by the source
(e.g., JSON, XML).
Below is a list of all available downloaders, including deprecated ones.
Detailed information about each downloader can be found in the downloader details section by following the links.

- [NVD CVE API](#nvd-cve-api)
- [NVD CPE API](#nvd-cpe-api)
- [MSRC](#msrc)
- [MSRC Security Guide](#msrc-security-guide)
- [GHSA](#ghsa)
- [CERT-SEI](#cert-sei)
- [CERT-FR](#cert-fr)
- [CERT-EU](#cert-eu)
- [CISA KEV](#cisa-kev)
- [EPSS](#epss)
- [EOL](#eol)

#### Deprecated Downloaders

- [_NVD (deprecated)_](#nvd-deprecated)
- [_CPE Dictionary (deprecated)_](#cpe-dictionary-deprecated)
- [_MSRC Manual CSV (deprecated)_](#msrc-manual-csv-deprecated)

## POM Configuration

The downloader configuration is part of the `ae-mirror-plugin` configuration in the Maven POM.
An example configuration with all available downloaders is provided below.

<details>

<summary>Click to show the full POM configuration</summary>

```xml
<plugins>
    <plugin>
        <groupId>com.metaeffekt.artifact.analysis</groupId>
        <artifactId>ae-mirror-plugin</artifactId>
        <version>${ae.artifact.analysis.version}</version>

        <executions>
            <execution>
                <id>data-mirror-download</id>
                <goals>
                    <goal>data-mirror</goal>
                </goals>

                <configuration>
                    <!-- General Configuration -->
                    <mirrorDirectory>${database.path}</mirrorDirectory>

                    <proxyScheme>${proxy.scheme}</proxyScheme>
                    <proxyHost>${proxy.host}</proxyHost>
                    <proxyUsername>${proxy.user}</proxyUsername>
                    <proxyPassword>${proxy.pass}</proxyPassword>
                    <proxyPort>${proxy.port}</proxyPort>

                    <!-- Downloads -->                    
                    <nvdCveDownload>
                      <resourceLocations>
                        <CVE_API_LIST_ALL>https://services.nvd.nist.gov/rest/json/cves/2.0?startIndex=%d</CVE_API_LIST_ALL>
                        <CVE_API_START_END_DATE>https://services.nvd.nist.gov/rest/json/cves/2.0/?startIndex=%d&amp;lastModStartDate=%s&amp;lastModEndDate=%s</CVE_API_START_END_DATE>
                      </resourceLocations>
                    </nvdCveDownload>
                    
                    <nvdCpeDownload>
                      <resourceLocations>
                        <CPE_API_LIST_ALL>https://services.nvd.nist.gov/rest/json/cpes/2.0?startIndex=%d</CPE_API_LIST_ALL>
                        <CPE_API_START_END_DATE>https://services.nvd.nist.gov/rest/json/cpes/2.0?startIndex=%d&amp;lastModStartDate=%s&amp;lastModEndDate=%s</CPE_API_START_END_DATE>
                        <CPE_MATCH_API_LIST_ALL>https://services.nvd.nist.gov/rest/json/cpematch/2.0?startIndex=%d</CPE_MATCH_API_LIST_ALL>
                        <CPE_MATCH_API_START_END_DATE>https://services.nvd.nist.gov/rest/json/cpematch/2.0?startIndex=%d&amp;lastModStartDate=%s&amp;lastModEndDate=%s</CPE_MATCH_API_START_END_DATE>
                      </resourceLocations>
                    </nvdCpeDownload>
                    
                    <msrcDownload>
                      <resourceLocations>
                        <MSRC_UPDATES_URL>https://api.msrc.microsoft.com/cvrf/v3.0/updates</MSRC_UPDATES_URL>
                        <MSRC_CVRF_BASE_URL>https://api.msrc.microsoft.com/cvrf/v3.0/cvrf/%s</MSRC_CVRF_BASE_URL>
                      </resourceLocations>
                    </msrcDownload>
                    
                    <msrcSecurityGuideDownload>
                      <resourceLocations>
                        <AFFECTED_PRODUCTS_BASE_URL>https://api.msrc.microsoft.com/sug/v2.0/en-US/affectedProduct</AFFECTED_PRODUCTS_BASE_URL>
                      </resourceLocations>
                    </msrcSecurityGuideDownload>
                    
                    <githubAdvisorDownload>
                      <resourceLocations>
                        <GHSA_GIT_URL>https://github.com/github/advisory-database</GHSA_GIT_URL>
                        <GHSA_GIT_ZIP_DOWNLOAD_URL>https://github.com/github/advisory-database/archive/refs/heads/main.zip</GHSA_GIT_ZIP_DOWNLOAD_URL>
                      </resourceLocations>
                    </githubAdvisorDownload>
                    
                    <certSeiDownload>
                      <resourceLocations>
                        <SUMMARY_API_URL>https://kb.cert.org/vuls/api/%d/summary/</SUMMARY_API_URL>
                        <NOTES_API_URL>https://kb.cert.org/vuls/api/%s/</NOTES_API_URL>
                      </resourceLocations>
                    </certSeiDownload>
                    
                    <certFrDownload>
                      <resourceLocations>
                        <CERT_FR_ARCHIVE>https://metaeffekt.com/mirror/cert-fr/txt/%d.tar</CERT_FR_ARCHIVE>
                        <CERT_FR_RSS_FEED>https://www.cert.ssi.gouv.fr/feed/</CERT_FR_RSS_FEED>
                        <CERT_FR_ENTRY_LISTING_HTML>https://www.cert.ssi.gouv.fr/%s/page/%d</CERT_FR_ENTRY_LISTING_HTML>
                        <CERT_FR_BASE_URL_FOR_ARCHIVE_LISTING>https://www.cert.ssi.gouv.fr</CERT_FR_BASE_URL_FOR_ARCHIVE_LISTING>
                        <CERT_FR_SINGLE_JSON_ENTRY>https://www.cert.ssi.gouv.fr/%s/%s/json</CERT_FR_SINGLE_JSON_ENTRY>
                      </resourceLocations>
                    </certFrDownload>
                    
                    <certEuDownload>
                      <resourceLocations>
                        <YEARLY_PUBLICATIONS_URL>https://cert.europa.eu/publications/security-advisories/%d</YEARLY_PUBLICATIONS_URL>
                        <SINGLE_ENTRY_URL>https://cert.europa.eu/publications/security-advisories/%s/json</SINGLE_ENTRY_URL>
                        <RSS_FEED>https://cert.europa.eu/publications/security-advisories-rss</RSS_FEED>
                      </resourceLocations>
                    </certEuDownload>
                    
                    <cisaKevDownload>
                      <resourceLocations>
                        <CISA_KEV>https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json</CISA_KEV>
                      </resourceLocations>
                    </cisaKevDownload>
                    
                    <epssDownload>
                      <resourceLocations>
                        <EPSS_ALL_SCORES_CSV>https://epss.cyentia.com/epss_scores-current.csv.gz</EPSS_ALL_SCORES_CSV>
                        <EPSS_SPECIFIC_DATE_CSV>https://epss.cyentia.com/epss_scores-%s-%s-%s.csv.gz</EPSS_SPECIFIC_DATE_CSV>
                      </resourceLocations>
                    </epssDownload>
                    
                    <eolDownload>
                      <resourceLocations>
                        <EOL_PRODUCT_VERSIONS>https://endoflife.date/api/%s.json</EOL_PRODUCT_VERSIONS>
                        <EOL_ALL_PRODUCTS>https://endoflife.date/api/all.json</EOL_ALL_PRODUCTS>
                      </resourceLocations>
                    </eolDownload>

                </configuration>
            </execution>
        </executions>
    </plugin>
</plugins>
```

</details>

<details>

<summary>Click to show the shortened POM configuration</summary>

```xml
<plugins>
    <plugin>
        <groupId>com.metaeffekt.artifact.analysis</groupId>
        <artifactId>ae-mirror-plugin</artifactId>
        <version>${ae.artifact.analysis.version}</version>

        <executions>
            <execution>
                <id>data-mirror-download</id>
                <goals>
                    <goal>data-mirror</goal>
                </goals>

                <configuration>
                    <!-- General Configuration -->
                    <mirrorDirectory>${database.path}</mirrorDirectory>

                    <proxyScheme>${proxy.scheme}</proxyScheme>
                    <proxyHost>${proxy.host}</proxyHost>
                    <proxyUsername>${proxy.user}</proxyUsername>
                    <proxyPassword>${proxy.pass}</proxyPassword>
                    <proxyPort>${proxy.port}</proxyPort>

                    <!-- Downloads -->
                    <nvdCveDownload/>
                    <nvdCpeDownload/>
                    <msrcDownload/>
                    <msrcSecurityGuideDownload/>
                    <githubAdvisorDownload/>
                    <certSeiDownload/>
                    <certFrDownload/>
                    <certEuDownload/>
                    <cisaKevDownload/>
                    <epssDownload/>
                    <eolDownload/>

                </configuration>
            </execution>
        </executions>
    </plugin>
</plugins>
```

</details>

## General Downloading Workflow

Each downloader operates independently and can be executed using the `data-mirror` goal from the
`com.metaeffekt.artifact.analysis:ae-mirror-plugin`.
A full configuration is provided in the [POM Configuration](#pom-configuration) section.

While the downloaders have distinct implementations, they all follow a similar overall process.
Before actually running a download when triggered, they check if an update is needed.
The download step is skipped if no update is required.

A download is considered necessary if:

- The downloader has never been run before.
- The previous download attempt failed.
- The data is older than a configured threshold.
- The remote source has been updated since the last download. (not available for all downloaders)

Each downloader stores then its data in a single designated directory, with a format that depends on the downloader.

![Download Process](download-process.svg)

The remote resource locations can be configured by using the ResourceLocations configuration.
Every downloader makes its remote URLs available via this configuration, allowing hosting the data on different servers,
like a local mirror or a caching proxy.
All available resource locations are listed in the detailed downloader descriptions.

# Downloader Details

## NVD CVE API

`nvd` / `nvdCveDownload`

<details>

<summary>Source Code for <code>NvdCveApiDownload.performDownload()</code> </summary>

```java
final boolean fullMirrorRequired = isFullMirrorRequired();

if (fullMirrorRequired) {
    clearDownload();
    LOG.info("Downloading initial NVD data from NVD API");
} else {
    LOG.info("Existing mirror detected. Downloading incremental NVD data from NVD API");
}

LOG.info("Requests {} be authorized with an API key, delay between requests [{}]",
        apiKey == null ? "will not" : "will",
        TimeUtils.formatTimeDiff(apiKey == null ? API_DELAY_BETWEEN_UNAUTHORIZED_REQUESTS : API_DELAY_BETWEEN_AUTHORIZED_REQUESTS));

final long lastModified = getDownloadDirectoryLastModified();
final Date lastModifiedDate = new Date(lastModified);
final Date now = new Date(TimeUtils.utcNow());

int currentStartIndex = 0;

final List<JSONArray> apiResponseDataToBeProcessed = new ArrayList<>();

while (true) {
    final long processingStartTime = System.currentTimeMillis();

    int finalCurrentStartIndex = currentStartIndex;
    final AtomicReference<JSONObject> json = new AtomicReference<>();
    new Retry(() -> {
        if (fullMirrorRequired) {
            json.set(downloadCvePage(finalCurrentStartIndex));
        } else {
            json.set(downloadCvePage(finalCurrentStartIndex, lastModifiedDate, now));
        }
    })
            .withDelay(API_DELAY_BETWEEN_UNAUTHORIZED_REQUESTS)
            .onException(Exception.class)
            .retryCount(8)
            .run();

    final int totalResults = json.get().getInt("totalResults");
    final int resultsPerPage = json.get().getInt("resultsPerPage");
    currentStartIndex += resultsPerPage;

    if (totalResults == 0) {
        LOG.info("No CVEs found for the given range.");
        break;
    }

    LOG.info("Downloaded CVEs [{}] to [{}] of [{}] [{} %]", currentStartIndex - resultsPerPage, currentStartIndex, totalResults, (currentStartIndex * 100 / totalResults));

    final JSONArray vulnerabilities = json.get().getJSONArray("vulnerabilities");
    apiResponseDataToBeProcessed.add(vulnerabilities);

    if (apiResponseDataToBeProcessed.size() >= 10) {
        final JSONArray merged = this.mergeCveItems(apiResponseDataToBeProcessed);
        this.processApiCveItems(merged);
        apiResponseDataToBeProcessed.clear();
    }

    final long processingDuration = System.currentTimeMillis() - processingStartTime;

    if (currentStartIndex >= totalResults) {
        break;
    }

    try {
        final long sleepDuration = (apiKey == null ? API_DELAY_BETWEEN_UNAUTHORIZED_REQUESTS : API_DELAY_BETWEEN_AUTHORIZED_REQUESTS) - processingDuration;
        if (sleepDuration > 0) {
            LOG.info("Sleeping for [{}] to avoid rate limit", TimeUtils.formatTimeDiff(sleepDuration));
            Thread.sleep(sleepDuration);
        }
    } catch (InterruptedException e) {
    }
}

if (apiResponseDataToBeProcessed.size() > 0) {
    final JSONArray merged = this.mergeCveItems(apiResponseDataToBeProcessed);
    this.processApiCveItems(merged);
}

LOG.info("Finished processing all CVEs");
```

</details>


<p>References:</p>
<ul>
    <li>API endpoint reference: <a href="https://nvd.nist.gov/developers/vulnerabilities">Vulnerability API</a></li>
    <li>API functionality overview: <a href="https://nvd.nist.gov/developers/start-here">Developers - Start here</a></li>
    <li>API workflow: <a href="https://nvd.nist.gov/developers/api-workflows">API User Workflow</a></li>
    <li>Request an API Key: <a href="https://nvd.nist.gov/developers/request-an-api-key">NVD - API Request</a></li>
</ul>
<p>
The new NVD API provides access to CVE and CPE. This download will use the CVE API to create one JSON file per year from
1999 to the current year. Note that you will need to
<a href="https://nvd.nist.gov/developers/request-an-api-key">request an API Key</a> to increase the rate limit:
</p>
<ul>
    <li>without key: <strong>5</strong> requests in a rolling <strong>30</strong>-second window (delay: 6400ms)</li>
    <li>with key: <strong>50</strong> requests in a rolling <strong>30</strong>-second window (delay: 600ms)</li>
</ul>
<p>
The API works on a pagination principle: It will only return the first (CVE: 2000, CPE: dict=10000/match=5000) matching
entries from a query, for the rest, further requests with a <code>startIndex</code> parameter have to be made. The different
amounts of results per request have most likely been picked that way, so that all data sources require roughly the same
amount of requests, which is currently (2023-02) for all three of them ~100.
</p><p>
The advantages of this new API are not seen on an initial mirror (it takes quite a bit longer, actually), but rather on
successive calls, where only few new/updated vulnerabilities are pulled using the <code>lastModStartDate</code> and
<code>lastModEndDate</code> parameters.
</p>
<p>The actual implementation of this works on a supplier-consumer pattern. It starts multiple threads that will
continuously make requests to the API (as long as there are requests to be made) and append those to a list of cached
JSON responses. As soon as this list reaches a size of 10 or if the end is reached, a consumer thread will take these
and sort them into yearly JSON files, where they are merged with existing ones in case of an update or appended in case
of a new vulnerability.</p>
<p>This multi-file approach drastically reduces the amount of searching and loading of vulnerabilities when checking
whether the vulnerability already exists. The threshold of 10 requests has been picked as it showed to be the most
memory/speed efficient one.</p>
<p>In the end, files from 1999 to the current year will be present in the download directory:</p>
<pre><code>.
├── <span class="hljs-number">1999</span><span class="hljs-selector-class">.json</span>
├── <span class="hljs-number">2000</span><span class="hljs-selector-class">.json</span>
├── <span class="hljs-number">2001</span><span class="hljs-selector-class">.json</span>
├── <span class="hljs-number">2002</span><span class="hljs-selector-class">.json</span>
...
├── <span class="hljs-number">2022</span><span class="hljs-selector-class">.json</span>
└── <span class="hljs-number">2023</span>.json
</code></pre>

| Resource Location | Description |
| --- | --- |
| `CVE_API_LIST_ALL` | <p>https://services.nvd.nist.gov/rest/json/cves/2.0?startIndex=%d</p><ol>     <li><code>startIndex</code> 0-based index of the first CVE to be returned in the response data</li> </ol> |
| `CVE_API_START_END_DATE` | <p>https://services.nvd.nist.gov/rest/json/cves/2.0/?startIndex=%d&lastModStartDate=%s&lastModEndDate=%s</p>The maximum allowable range when using any date range parameters is 120 consecutive days.<br> Values must be entered in the extended ISO-8061 date/time format:<br> <code>[YYYY][“-”][MM][“-”][DD][“T”][HH][“:”][MM][“:”][SS][Z]</code> <ol>     <li><code>startIndex</code> 0-based index of the first CVE to be returned in the response data</li>     <li><code>lastModStartDate</code> the start date</li>     <li><code>lastModEndDate</code> the end date</li> </ol> |


## NVD CPE API

`cpe-dict` / `nvdCpeDownload`

<details>

<summary>Source Code for <code>NvdCpeApiDownload.performDownload()</code> </summary>

```java
final boolean fullMirrorRequired = isFullMirrorRequired();

if (fullMirrorRequired) {
    LOG.info("Downloading initial NVD data from NVD API");
} else {
    LOG.info("Existing mirror detected. Downloading incremental NVD data from NVD API");
}

final long baseSleepDuration = apiKey == null ? API_DELAY_BETWEEN_UNAUTHORIZED_REQUESTS : API_DELAY_BETWEEN_AUTHORIZED_REQUESTS;

super.executor.setSize(4);
super.executor.setDelay(baseSleepDuration);

LOG.info("Requests {} be authorized with an API key, delay between requests [{}]",
        apiKey == null ? "will not" : "will", TimeUtils.formatTimeDiff(baseSleepDuration));

final long lastModified = getDownloadDirectoryLastModified();
final Date lastModifiedDate = new Date(lastModified);
final Date now = new Date(TimeUtils.utcNow());

downloadCpeDictionaryApiData(fullMirrorRequired, lastModifiedDate, now);
downloadCpeMatchApiData(fullMirrorRequired, lastModifiedDate, now);
```

</details>


<p>See the NVD CVE download for more details on the general data format and references of the data source.
The data feed specific to the CPE data is split into two parts: The CPE Dictionary and the CPE Matches with each their own endpoint:</p>
<ul>
    <li>dictionary: <a href="https://services.nvd.nist.gov/rest/json/cpes/2.0">https://services.nvd.nist.gov/rest/json/cpes/2.0</a><br>
    allows 10000 results per request: ~1000000 CPE entries --&gt; ~100 requests<br>
    The dictionary contains a list of CPE that can be used to identify products. However, (most of) these CPEs do not contain any version information.
    </li>
    <li>match: <a href="https://services.nvd.nist.gov/rest/json/cpematch/2.0">https://services.nvd.nist.gov/rest/json/cpematch/2.0</a><br>
    allows 5000 results per request: ~450000 CPE entries --&gt; ~90 requests<br>
    The CPE match contains the versions that are missing from the dictionary. It contains almost no new CPEs, but the version information is added to the previously found CPEs.
    </li>
</ul>
<p>Since our version matching algorithm does not rely solely on the versions provided by the CPE entries,
the relations between the different entries are not relevant in our context and the hierarchical structure from dict/match is flattened,
normalized and stored in a single data structure.</p>
<p>The relevant keys that are stored in the local files for each CPE are <code>cpeName</code>, <code>cpeNameId</code>, <code>lastModified</code>, <code>created</code> and
<code>deprecated</code>. Additionally, they can have <code>titles</code> and <code>refs</code> for several titles in different languages and references
with a title and a link.</p>

| Resource Location | Description |
| --- | --- |
| `CPE_API_LIST_ALL` | <p>https://services.nvd.nist.gov/rest/json/cpes/2.0?startIndex=%d</p><ol>     <li><code>startIndex</code> 0-based index of the first CPE to be returned in the response data</li> </ol> |
| `CPE_API_START_END_DATE` | <p>https://services.nvd.nist.gov/rest/json/cpes/2.0?startIndex=%d&lastModStartDate=%s&lastModEndDate=%s</p>The maximum allowable range when using any date range parameters is 120 consecutive days.<br> Values must be entered in the extended ISO-8061 date/time format:<br> <code>[YYYY][“-”][MM][“-”][DD][“T”][HH][“:”][MM][“:”][SS][Z]</code> <ol>     <li><code>startIndex</code> 0-based index of the first CPE to be returned in the response data</li>     <li><code>lastModStartDate</code> the start date</li>     <li><code>lastModEndDate</code> the end date</li> </ol> |
| `CPE_MATCH_API_LIST_ALL` | <p>https://services.nvd.nist.gov/rest/json/cpematch/2.0?startIndex=%d</p><ol>     <li><code>startIndex</code> 0-based index of the first CPE to be returned in the response data</li> </ol> |
| `CPE_MATCH_API_START_END_DATE` | <p>https://services.nvd.nist.gov/rest/json/cpematch/2.0?startIndex=%d&lastModStartDate=%s&lastModEndDate=%s</p>The maximum allowable range when using any date range parameters is 120 consecutive days.<br> Values must be entered in the extended ISO-8061 date/time format:<br> <code>[YYYY][“-”][MM][“-”][DD][“T”][HH][“:”][MM][“:”][SS][Z]</code> <ol>     <li><code>startIndex</code> 0-based index of the first CPE to be returned in the response data</li>     <li><code>lastModStartDate</code> the start date</li>     <li><code>lastModEndDate</code> the end date</li> </ol> |


## MSRC

`msrc` / `msrcDownload`

<details>

<summary>Source Code for <code>MsrcDownload.performDownload()</code> </summary>

```java
final Map<String, String> changedMonths = getChangedMonths();
LOG.info("Months with changed/missing contents: {}", changedMonths);

for (Map.Entry<String, String> entry : changedMonths.entrySet()) {
    final String monthId = entry.getKey();
    final String currentReleaseDate = entry.getValue();

    super.executor.submit(() -> {
        final String year = monthId.substring(0, 4);
        final File downloadDestinationParentDirectory = new File(super.downloadIntoDirectory, year);
        final File downloadDestinationFile = new File(downloadDestinationParentDirectory, monthId + ".xml");

        if (!downloadDestinationParentDirectory.exists()) {
            downloadDestinationParentDirectory.mkdirs();
        }

        if (downloadDestinationFile.exists()) {
            downloadDestinationFile.delete();
        }

        final URL requestUrl = getRemoteResourceLocationUrl(MSRC_CVRF_BASE_URL, monthId);

        super.downloader.fetchResponseBodyFromUrlToFile(requestUrl, downloadDestinationFile);

        super.propertyFiles.set(super.downloadIntoDirectory, "info", InfoFileAttributes.MSRC_PREFIX.getKey() + "month-latest-" + monthId, currentReleaseDate);
    });
}

super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for download threads to finish.", e);
}
```

</details>


<p>References:</p>
<ul>
<li>MSRC Homepage: <a href="https://msrc.microsoft.com/">Microsoft Security Response Center</a></li>
<li>MSRC CVRF API: <a href="https://api.msrc.microsoft.com/cvrf/v3.0/swagger/index">Microsoft Security Updates API</a></li>
</ul>
<p>The MSRC (Microsoft Security Response Center) provides a REST-API for their Common Vulnerability Reporting Framework (CVRF) v3.0.
This API can be used to download all advisories published by the MSRC. The documents provided are security advisories
that can be used to 1. identify vulnerabilities using microsoft product ids and 2. to provide information on how to
mitigate these vulnerabilities.</p>
<p>The MSRC splits their data into three sources, this downloader accesses one of the two sources that can be queried automatically.
Individual advisory entries are each stored in a separate XML file, to which their relative paths are all listed in a single data feed.
In order to perform a download for this particular source, the following steps are executed:</p>
<ol>
<li>All documents available are contained within the XML document available via
<a href="https://api.msrc.microsoft.com/cvrf/v3.0/updates">https://api.msrc.microsoft.com/cvrf/v3.0/updates</a>. This document
contains multiple <code>CvrfUrl</code> tags, that are collected into a list. The MSRC structures their advisors into one
document per month, so each URL will lead to a different monthly XML file.</li>
<li>The downloader checks whether it already knows these documents/whether they are up-to-date.</li>
<li>All unknown/incomplete <code>CvrfUrl</code> URLs are downloaded.</li>
</ol>
<p>These files are stored in one directory per year (starting 2016), with paths in the form of <code>[YEAR]/[YEAR]-[MONTH].xml</code>.</p>
<pre><code>.
├── <span class="hljs-number">2016</span>
│   ├── <span class="hljs-number">2016</span>-Apr.<span class="hljs-keyword">xml</span>
<span class="hljs-title">│   ├── 2016-Aug</span>.<span class="hljs-keyword">xml</span>
<span class="hljs-title">...
└── 2023</span>
    └── <span class="hljs-number">2023</span>-Jan.xml
</code></pre>

| Resource Location | Description |
| --- | --- |
| `MSRC_UPDATES_URL` | <p>https://api.msrc.microsoft.com/cvrf/v3.0/updates</p>List of all monthly documents. Contains last update date used to determine whether an update is required.<br> When viewed in a browser, the content will be displayed as XML, but when accessed via the API, the content will be returned as JSON. |
| `MSRC_CVRF_BASE_URL` | <p>https://api.msrc.microsoft.com/cvrf/v3.0/cvrf/%s</p><a href="https://api.msrc.microsoft.com/cvrf/v3.0/swagger/v3/swagger.json">https://api.msrc.microsoft.com/cvrf/v3.0/swagger/v3/swagger.json</a>. REST-API for the Microsoft Security Response Center (MSRC) Common Vulnerability Reporting Framework (CVRF) v2.0. <ol>     <li><code>%s</code> CVRF document ID (yyyy-mmm) (example: <code>2021-dec</code>)</li> </ol> |


## MSRC Security Guide

`msrc-security-guide` / `msrcSecurityGuideDownload`

<details>

<summary>Source Code for <code>MsrcSecurityGuideDownload.performDownload()</code> </summary>

```java
final String startDate = determineStartDate();
final String endDate = getCurrentTimestampForRequest();

downloadAllFromTo(startDate, endDate);
```

</details>


<details>

<summary>Source Code for <code>MsrcSecurityGuideDownload.downloadAllFromTo(String startDate, String endDate)</code> </summary>

```java
int offset = 0;
int totalEntriesCount = 0;
int currentEntriesCount = 0;
int currentRequestsSinceLastWriteToFile = 0;
long startTime = System.currentTimeMillis();

final Map<String, List<JSONObject>> entriesToBeWrittenToFile = new ConcurrentHashMap<>();

super.executor.setDelay(0);

do {
    try {
        final JSONObject response = downloadSecurityGuideByDateRangeAndOffset(startDate, endDate, offset);
        if (isResponseEmpty(response)) {
            LOG.info("No more entries found. Stopping download.");
            break;
        }

        totalEntriesCount = response.getInt("@odata.count");
        currentEntriesCount = response.getJSONArray("value").length();
        offset += currentEntriesCount;
        currentRequestsSinceLastWriteToFile++;

        sortEntriesIntoYears(response, entriesToBeWrittenToFile);

        if (currentRequestsSinceLastWriteToFile >= WRITE_TO_FILE_THRESHOLD) {
            final long currentTime = System.currentTimeMillis();
            final long elapsedTime = currentTime - startTime;
            final long expectedRemainingTime = (elapsedTime / offset) * (totalEntriesCount - offset);

            LOG.info("Downloaded [{} / {}] entries, expected time remaining [{}]", offset, totalEntriesCount, TimeUtils.formatTimeDiff(expectedRemainingTime));

            super.executor.submit(() -> {
                try {
                    writeEntriesToFile(entriesToBeWrittenToFile);
                } catch (IOException e) {
                    throw new RuntimeException("Failed to write entries to file", e);
                }
            });
            super.executor.start();
            currentRequestsSinceLastWriteToFile = 0;
        }
    } catch (IOException e) {
        LOG.error("Error while downloading security guide.", e);
        break;
    }
} while (offset < totalEntriesCount);

try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to join executor.", e);
}

try {
    writeEntriesToFile(entriesToBeWrittenToFile);
} catch (IOException e) {
    LOG.error("Error while writing entries to file.", e);
}

try {
    setPreviousTotalEntriesCount(getTotalAvailableEntriesCount());
} catch (IOException e) {
    setPreviousTotalEntriesCount(totalEntriesCount);
}
setLastMirrorDate(endDate);
```

</details>


<details>

<summary>Source Code for <code>MsrcSecurityGuideDownload.downloadSecurityGuideByDateRangeAndOffset(String startDate, String endDate, int offset)</code> </summary>

```java
return new Retry<>(() -> {
    final Map<String, String> getParameters = new HashMap<>();

    getParameters.put("$orderBy", "releaseDate asc");
    getParameters.put("$filter", "(releaseDate gt " + startDate + ") and (releaseDate lt " + endDate + ")");
    getParameters.put("$skip", String.valueOf(offset));

    final String getRequestUrl = super.downloader.buildGetRequest(getRemoteResourceLocation(AFFECTED_PRODUCTS_BASE_URL), getParameters);
    final List<String> response = super.downloader.fetchResponseBodyFromUrlAsList(new URL(getRequestUrl));

    return new JSONObject(String.join("", response));
})
        .onException(Throwable.class)
        .withValidator(json -> { // must not be {"error":{"code":"400","message":"..."}}
            if (json.has("error")) {
                final JSONObject error = json.getJSONObject("error");
                final String code = error.getString("code");
                final String message = error.getString("message");
                LOG.error("Error while downloading security guide: [{}] {}", code, message);
                return false;
            }
            return true;
        })
        .retryCount(5)
        .withDelay(1000 * 5)
        .run();
```

</details>


<p>References:</p>
<ul>
<li>Update Guide/CSV download: <a href="https://msrc.microsoft.com/update-guide">https://msrc.microsoft.com/update-guide</a></li>
</ul>
<p>As mentioned in the MSRC Download, the MSRC splits its data into three separate data sources.
This particular data source used to only be available via manual CSV file downloads, but since then, a way was found to perform these requests automatically.</p>
<p>Another thing to investigate is this new API (as of 2024-09-07) they made available for use:
<a href="https://api.msrc.microsoft.com/cvrf/v3.0/swagger/v3/swagger.json">https://api.msrc.microsoft.com/cvrf/v3.0/swagger/v3/swagger.json</a>
and whether it can replace this existing one once more.</p>

| Resource Location | Description |
| --- | --- |
| `AFFECTED_PRODUCTS_BASE_URL` | <p>https://api.msrc.microsoft.com/sug/v2.0/en-US/affectedProduct</p>Endpoint that <a href="https://msrc.microsoft.com/update-guide/">https://msrc.microsoft.com/update-guide/</a> uses to construct the xlsx and csv files locally.<br> Has parameters that are filled by the code, without format strings here. |


## GHSA

`github-advisory-database` / `githubAdvisorDownload`

<details>

<summary>Source Code for <code>GitRepositoryDownload.performDownload()</code> </summary>

```java
if (!getDownloadIntoDirectory().exists()) {
    getDownloadIntoDirectory().mkdirs();
}
final File dir = new File(getDownloadIntoDirectory(), getRepositoryName());

if (useGitCommand) {
    LOG.info("Using git command to clone repository");
    performGitDownload(dir);
} else {
    LOG.info("Using zip download to download repository");
    performZipDownload(dir);
}

LOG.info("Repository updated");
```

</details>


<details>

<summary>Source Code for <code>GitRepositoryDownload.performGitDownload(File dir)</code> </summary>

```java
final boolean gitLogExists = new File(dir, ".git").exists();

if (gitLogExists) {
    try {
        LOG.info("Updating repository from {} into {}", getRepositoryUrl(), dir.getAbsolutePath());
        super.git.pull(dir);
    } catch (IOException e) {
        throw new RuntimeException("Failed to pull repository updates from " + getRepositoryUrl() + " into " + dir.getAbsolutePath(), e);
    }

} else {
    try {
        LOG.info("Cloning repository from {} into {}", getRepositoryUrl(), dir.getAbsolutePath());
        super.git.cloneRemote(dir, getRepositoryUrl(), null);
    } catch (IOException e) {
        throw new RuntimeException("Failed to clone repository from " + getRepositoryUrl() + " into " + dir.getAbsolutePath(), e);
    }
}
```

</details>


<details>

<summary>Source Code for <code>GitRepositoryDownload.performZipDownload(File dir)</code> </summary>

```java
final File zipFile = new File(getDownloadIntoDirectory(), getRepositoryName() + ".zip");

LOG.info("[long operation] Downloading [{}] into [{}]", getZipDownloadUrl(), zipFile.getAbsolutePath());
super.downloader.fetchResponseBodyFromUrlToFile(getZipDownloadUrl(), zipFile);

if (dir.exists()) {
    LOG.info("[long operation] Deleting existing directory: {}", dir.getAbsolutePath());
    try {
        FileUtils.deleteDirectory(dir);
    } catch (IOException e) {
        try {
            Thread.sleep(1000);
            FileUtils.deleteDirectory(dir);
        } catch (IOException | InterruptedException ex) {
            throw new RuntimeException("Failed to delete previous download in " + dir.getAbsolutePath(), e);
        }
    }
}

if (zipFile.exists()) {
    LOG.info("[long operation] Unpacking [{}] into [{}]", zipFile.getAbsolutePath(), dir.getParentFile().getAbsolutePath());
    ArchiveUtils.unpackIfPossible(zipFile, dir.getParentFile(), new ArrayList<>());
} else {
    throw new RuntimeException("Failed to download " + getZipDownloadUrl() + " into " + zipFile.getAbsolutePath());
}

// find the single directory that starts with the target download name and rename it to the target directory
final File[] files = dir.getParentFile().listFiles();
if (files != null) {
    for (File file : files) {
        if (file.isDirectory() && file.getName().startsWith(getRepositoryName())) {
            LOG.info("Renaming [{}] to [{}]", file.getAbsolutePath(), dir.getAbsolutePath());
            file.renameTo(dir);
            break;
        }
    }
}

if (!dir.exists()) {
    throw new RuntimeException("Failed to unpack " + zipFile.getAbsolutePath() + " into " + dir.getAbsolutePath());
}

LOG.info("Deleting [{}]", zipFile.getAbsolutePath());
zipFile.delete();
```

</details>


<p>References:</p>
<ul>
<li>GitHub Security Advisory Database: <a href="https://github.com/advisories">https://github.com/advisories</a></li>
</ul>
<p>GitHub provides a service for project maintainers to publish their own security advisories for their projects. This service
uses the <a href="https://osv.dev/">OSV (Open Source Vulnerabilities)</a> schema to provide structured data on vulnerabilities
in open source projects. The data is provided in a git repository, which can be cloned or downloaded as a zip file, depending on the <code>useGitCommand</code> parameter.</p>
<p>The downloader will create a directory <code>github-advisory-database</code> in the <code>download</code> directory, where a directory
structure will be created that mirrors the structure of the repository.</p>
<p>The advisories are further split into two directories: <code>advisories/github-reviewed</code> and <code>advisories/unreviewed</code>.
Each file is contained within a separate directory, with the name of the directory being the advisory ID.
A flag can later be used to indicate to only use reviewed advisories or to also include the unreviewed ones.</p>
<p>Their schema being the OSV schema, a generalized approach could be useful to mirror potentially many different OSV data sources into the same data structure.</p>
<pre><code>.
└── advisories
    ├── github-reviewed
    │   ├── <span class="hljs-number">2017</span>
    │   │   ├── <span class="hljs-number">10</span>
    │   │   │   ├── GHSA<span class="hljs-number">-229</span>r-pqp6<span class="hljs-number">-8</span>w6g
    │   │   │   │   └── GHSA<span class="hljs-number">-229</span>r-pqp6<span class="hljs-number">-8</span>w6g.json
    │   │   │   ├── GHSA<span class="hljs-number">-24</span>fg-p96v-hxh8
    │   │   │   │   └── GHSA<span class="hljs-number">-24</span>fg-p96v-hxh8.json
    └── unreviewed
        ├── <span class="hljs-number">2021</span>
        │   ├── <span class="hljs-number">04</span>
        │   │   └── GHSA-m5pg<span class="hljs-number">-8</span>h68-j225
        │   │       └── GHSA-m5pg<span class="hljs-number">-8</span>h68-j225.json
</code></pre>

| Resource Location | Description |
| --- | --- |
| `GHSA_GIT_URL` | <p>https://github.com/github/advisory-database</p>Remote repository URL of the <a href="https://github.com/github/advisory-database">github/advisory-database</a> repository. |
| `GHSA_GIT_ZIP_DOWNLOAD_URL` | <p>https://github.com/github/advisory-database/archive/refs/heads/main.zip</p>Remote URL of the zip download of the <a href="https://github.com/github/advisory-database">github/advisory-database</a> repository. |


## CERT-SEI

`certsei` / `certSeiDownload`

<details>

<summary>Source Code for <code>CertSeiDownload.performDownload()</code> </summary>

```java
if (cachedChangedYearlySummaryVUIDs == null) {
    cachedChangedYearlySummaryVUIDs = fetchChangedYearlySummaryVUIDs();
}

for (Map.Entry<Integer, List<String>> yearVUIDs : cachedChangedYearlySummaryVUIDs.entrySet()) {
    final int year = yearVUIDs.getKey();
    final File yearDirectory = new File(super.downloadIntoDirectory, String.valueOf(yearVUIDs.getKey()));

    for (final String note : yearVUIDs.getValue().stream().sorted().collect(Collectors.toList())) {
        super.executor.submit(() -> {
            final String idNumber = note.replace("VU#", "");
            final URL requestUrl = getRemoteResourceLocationUrl(NOTES_API_URL, idNumber);
            final File noteFile = new File(yearDirectory, note + ".json");

            long remoteFileSize = super.downloader.fetchFileSizeFromUrl(requestUrl);

            if (!noteFile.exists() || noteFile.length() != remoteFileSize) {
                LOG.info("Fetching note [{}] for year [{}]", note, yearVUIDs.getKey());
                new Retry<>(() -> super.downloader.fetchResponseBodyFromUrlToFile(requestUrl, noteFile))
                        .withValidator((result) -> {
                            if (!noteFile.exists()) {
                                LOG.warn("Note file does not exist: {}", noteFile.getAbsolutePath());
                                return false;
                            }
                            try {
                                return FileUtils.readLines(noteFile, StandardCharsets.UTF_8).get(0).startsWith("{");
                            } catch (Exception e) {
                                LOG.warn("Content of note file is not valid JSON: {}", noteFile.getAbsolutePath());
                                return false;
                            }
                        })
                        .onException(Exception.class)
                        .withDelay(5000)
                        .retryCount(10)
                        .run();
            } else {
                LOG.info("Note [{}] for year [{}] is already up-to-date", note, yearVUIDs.getKey());
            }
        });
    }

    final URL requestUrl = getRemoteResourceLocationUrl(SUMMARY_API_URL, year);
    long remoteFileSize = super.downloader.fetchFileSizeFromUrl(requestUrl);

    super.propertyFiles.set(super.downloadIntoDirectory, "info", InfoFileAttributes.CERT_SEI_PREFIX.getKey() + "summary-" + year, remoteFileSize);
}

super.executor.setDelay(5);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for executor to finish", e);
}

cachedChangedYearlySummaryVUIDs = null;
```

</details>


<p>References:</p>
<ul>
<li>Data provider: <a href="https://www.sei.cmu.edu/">Software Engineering Institute</a></li>
<li>API reference:
<a href="https://vuls.cert.org/confluence/display/VIN/Vulnerability+Note+API">Vulnerability Note API - VINCE - VulWiki</a></li>
</ul>
<p>In order for the downloader to find what documents exist and need to be downloaded, the summary-API is used.
These summaries are provided by year and in a JSON format.
The starting year is 2000, which is why all summaries are downloaded from 2000 to the current year.</p>
<p>These summaries contain all notes (advisors) that were published in that year. The ID of each note is used to download
the individual notes.</p>
<p>To store the files locally, one directory per year is created, where the advisors are written to. The filename is
<code>VU#[ID].json</code>.</p>
<pre><code>.
├── <span class="hljs-number">2000</span>
│   ├── VU<span class="hljs-number">#111677</span><span class="hljs-selector-class">.json</span>
│   ├── VU<span class="hljs-number">#175</span>66<span class="hljs-selector-class">.json</span>
...
└── <span class="hljs-number">2022</span>
    ├── VU<span class="hljs-number">#119678</span><span class="hljs-selector-class">.json</span>
...
</code></pre>

| Resource Location | Description |
| --- | --- |
| `SUMMARY_API_URL` | <p>https://kb.cert.org/vuls/api/%d/summary/</p>Summary URL for a given year. <ol>     <li><code>%d</code> Summary year (example: <code>2020</code>)</li> </ol> |
| `NOTES_API_URL` | <p>https://kb.cert.org/vuls/api/%s/</p>URL for a certain note. <ol>     <li><code>%s</code> Note Identifier (example: <code>257161</code>)</li> </ol> |


## CERT-FR

`certfr` / `certFrDownload`

<details>

<summary>Source Code for <code>CertFrDownload.performDownload()</code> </summary>

```java
this.downloadRequiredArchiveYears();

super.propertyFiles.set(super.downloadIntoDirectory, "info", InfoFileAttributes.CERT_FR_PREFIX.getKey() + "last-feed-size", getCurrentFeedSize());
```

</details>


<details>

<summary>Source Code for <code>CertFrDownload.downloadRequiredArchiveYears()</code> </summary>

```java
final List<Integer> updateYears = determineWhatYearsRequireUpdate();

LOG.info("Updating year archives {}", updateYears);
if (updateYears.isEmpty()) {
    LOG.info("--> No archive years require update, skipping archive download");
    return;
}

for (Integer year : updateYears) {
    final File downloadIntoDirectory = super.downloadIntoDirectory;
    final WebAccess downloader = super.downloader;

    final ScheduledDelayedThreadPoolExecutor.ThrowingRunnable downloadThread = () -> {
        LOG.info("Updating year [{}]", year);

        final URL requestUrl = getRemoteResourceLocationUrl(CERT_FR_ARCHIVE, year);

        final File downloadToFile = new File(downloadIntoDirectory, year + ".tar");
        final File unpackedDirectory = new File(downloadIntoDirectory, year.toString());

        if (unpackedDirectory.exists()) {
            FileUtils.deleteDir(unpackedDirectory);
        }

        downloader.fetchResponseBodyFromUrlToFile(requestUrl, downloadToFile);

        if (!downloadToFile.exists()) {
            throw new RuntimeException("Download successful, but file does not exist: " + downloadToFile.getAbsolutePath() + " from: " + requestUrl);
        }
        if (downloadToFile.length() < 2000) {
            final String content = FileUtils.readFileToString(downloadToFile, StandardCharsets.UTF_8);
            if (content.contains("404") && (content.startsWith("{") || content.startsWith("[") || content.startsWith("<"))) {
                LOG.warn("Downloaded file contains 404, skipping extraction: {}", downloadToFile.getAbsolutePath());
                if (!downloadToFile.delete()) {
                    LOG.warn("Unable to delete downloaded archive file: {}", downloadToFile.getAbsolutePath());
                }
                return;
            }
        }

        try {
            ArchiveUtils.untar(downloadToFile, unpackedDirectory);
        } catch (Throwable e) {
            throw new RuntimeException("Unable to untar from " + downloadToFile + " to " + unpackedDirectory, e);
        }

        // ArchiveUtils#untar only throws exceptions if the untar failed and the following deletion of temporary
        // files fails, which is why the existence of the result has to be checked separately
        if (!unpackedDirectory.exists()) {
            if (!downloadToFile.delete()) {
                LOG.warn("Unable to delete downloaded archive file: {}", downloadToFile.getAbsolutePath());
            }

            throw new RuntimeException("Unable to untar from " + downloadToFile + " to " + unpackedDirectory);
        }

        if (!downloadToFile.delete()) {
            LOG.warn("Unable to delete downloaded archive file {}", downloadToFile.getAbsolutePath());
        }

        cleanFilesOfTypes(unpackedDirectory, pathname -> pathname.getName().endsWith(".pdf") || pathname.isDirectory() || pathname.getName().contains("XXX"));

        File[] extractedFiles = unpackedDirectory.listFiles();
        if (extractedFiles != null) {
            LOG.info("Extracted [{}] entries for year [{}]", extractedFiles.length, year);
        }
    };

    super.executor.submit(downloadThread);
}

super.executor.setDelay(3 * 1000);
super.executor.setSize(4);
super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for download threads to finish.", e);
}
```

</details>


<p>References:</p>
<ul>
<li>Data provider: <a href="https://www.cert.ssi.gouv.fr/">CERT-FR</a></li>
<li>{metaeffekt} mirror: https://metaeffekt.com/mirror/cert-fr/txt/%d.tar</li>
</ul>
<p>Up until the end of 2024-05, the CERT-FR provided yearly archives in the form of TAR files that contain all published security advisory notes in both a PDF and a TXT variant.
All files are simply extracted and put into a directory with the year as name.
All PDF files are discarded, as they are not machine-readable and therefore unusable for the indexing process.
The remaining TXT files are stored in files with a filename of <code>CERTA-[YEAR]-[TYPE]-[ID].txt</code>.
The advisories start from 2000, meaning that all tar files from 2000 to the current year are downloaded and extracted.</p>
<p>However, since then, they have changed their approach without warning the public of this change, with their transition period taking place over two weeks, where their files were not available for some time at all.
After that, the format completely changed, now being based on an API approach, where each file needs to be downloaded individually.
This results in over 18.000 requests that have to be made to obtain a full mirror.
This is not feasible on your client device, especially since there is no way to download only the updated notes, as they do not provide their "modified" information in the listing API.
So, the {metaeffekt} provides a service available on <code>https://metaeffekt.com/mirror/cert-fr/txt/%d.tar</code>, where we build the old tar files every night (CET) for you to download.
This is stored in the <code>CERT_FR_ARCHIVE</code> resource location.
As an alternative, JSON files are also built and uploaded to a separate directory: <code>https://metaeffekt.com/mirror/cert-fr/json/%d.tar</code>, which can be used as well for the download URL.</p>
<p>If you want to build this pre-stage and host it for yourself, you may use the <code>internal-data-mirror</code> goal of the <code>ae-mirror-plugin</code> as such and upload it via FTP (or other) to your sever.
Simply change the <code>CERT_FR_ARCHIVE</code> value to your domain.</p>
<pre lang="xml"><code>&lt;plugin&gt;
    &lt;groupId&gt;com.metaeffekt.artifact.analysis&lt;/groupId&gt;
    &lt;artifactId&gt;ae-mirror-plugin&lt;/artifactId&gt;
    &lt;version&gt;${ae.artifact.analysis.version}&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;internal-data-mirror&lt;/id&gt;
            &lt;goals&gt;
                &lt;goal&gt;internal-data-mirror&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
                &lt;mirrorDirectory&gt;${database.path}&lt;/mirrorDirectory&gt;
                &lt;certFrDownload&gt;&lt;/certFrDownload&gt;
                &lt;moveTargetFilesTo&gt;upload-to-server&lt;/moveTargetFilesTo&gt;
            &lt;/configuration&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;</code></pre>
<p>Another difficulty the TXT files pose on their own is that they come in a wildly unstructured format, being simple transcripts of the PDF files.
This makes parsing them later challenging. For more information on this, see the index process for the CERT-FR advisories.</p>
<pre><code>.
├── <span class="hljs-number">2000</span>
│   ├── CERTA<span class="hljs-number">-2000</span>-ALE<span class="hljs-number">-001.</span>txt
│   ├── CERTA<span class="hljs-number">-2000</span>-ALE<span class="hljs-number">-002.</span>txt
...
└── <span class="hljs-number">2022</span>
    ├── CERTFR<span class="hljs-number">-2022</span>-ACT<span class="hljs-number">-001.</span>txt
...
</code></pre>

| Resource Location | Description |
| --- | --- |
| `CERT_FR_ARCHIVE` | <p>https://metaeffekt.com/mirror/cert-fr/txt/%d.tar</p>Yearly archive download URL. Parameters: <ol>     <li><code>%d</code> Archive year (example: <code>2020</code>)</li> </ol> |
| `CERT_FR_RSS_FEED` | <p>https://www.cert.ssi.gouv.fr/feed/</p>RSS feed that contains the latest changes to the mirror.<br> Used to check if update is required. |
| `CERT_FR_ENTRY_LISTING_HTML` | <p>https://www.cert.ssi.gouv.fr/%s/page/%d</p>Paged listing of CERT-FR entries. Parameters: <ol>     <li><code>%s</code> Type of entry (example: <code>alerte</code>)</li>     <li><code>%d</code> Page number</li> </ol> Examples: <ul>     <li><a href="https://www.cert.ssi.gouv.fr/alerte/page/1">https://www.cert.ssi.gouv.fr/alerte/page/1</a></li>     <li><a href="https://www.cert.ssi.gouv.fr/avis/page/1">https://www.cert.ssi.gouv.fr/avis/page/2</a></li> </ul> |
| `CERT_FR_BASE_URL_FOR_ARCHIVE_LISTING` | <p>https://www.cert.ssi.gouv.fr</p>Base URL for the CERT-FR. Used to extract the archive year list. |
| `CERT_FR_SINGLE_JSON_ENTRY` | <p>https://www.cert.ssi.gouv.fr/%s/%s/json</p>Single JSON entry for a CERT-FR entry. Parameters: <ol>     <li><code>%s</code> Type of entry (example: <code>alerte</code>)</li>     <li><code>%s</code> Entry ID (example: <code>CERTFR-2021-ALE-020</code>)</li> </ol> Examples: <ul>     <li><a href="https://www.cert.ssi.gouv.fr/alerte/CERTFR-2021-ALE-020/json">https://www.cert.ssi.gouv.fr/alerte/CERTFR-2021-ALE-020/json</a></li> </ul> |


## CERT-EU

`certeu` / `certEuDownload`

<details>

<summary>Source Code for <code>CertEuDownload.performDownload()</code> </summary>

```java
final Map<Integer, List<String>> yearlyPublicationHtmlPages = getAllYearlyPublications();
final Map<Integer, Map<String, String>> entriesToBeFetchedPerYear = new LinkedHashMap<>();

for (Map.Entry<Integer, List<String>> yearPage : yearlyPublicationHtmlPages.entrySet()) {
    try {
        final Map<String, String> entriesToBeFetched = extractUpdatedEntryIdsForYearlyPublicationsHtml(yearPage.getKey(), yearPage.getValue());
        if (!entriesToBeFetched.isEmpty()) {
            entriesToBeFetchedPerYear.put(yearPage.getKey(), entriesToBeFetched);
        } else {
            log.info("Year [{}] is already complete and up to date, no need to fetch.", yearPage.getKey());
        }
    } catch (IOException e) {
        throw new RuntimeException("Failed to extract the yearly publication entries for CERT-EU on year " + yearPage.getKey(), e);
    }
}

if (entriesToBeFetchedPerYear.isEmpty()) {
    log.info("No new entries to fetch, skipping download.");
    return;
}

for (Map.Entry<Integer, Map<String, String>> yearEntries : entriesToBeFetchedPerYear.entrySet()) {
    final Map<String, String> entriesToBeFetched = yearEntries.getValue();
    final int year = yearEntries.getKey();
    log.info("Starting fetching process for year [{}] with [{}] entries to be fetched.", year, entriesToBeFetched.size());

    for (Map.Entry<String, String> entry : entriesToBeFetched.entrySet()) {
        final String entryId = entry.getKey();
        downloadEntry(year, entryId);
    }

    try {
        this.mergePreviouslyParsedEntryPublicationDatesBackIntoFile(entriesToBeFetched);
    } catch (IOException e) {
        log.error("Failed to merge the fetched entries back into the publication dates file, this means that next time, all entries for year [{}] will be fetched again. This error should not occur, as it is a simple file read/write access.", year, e);
    }
}
```

</details>


<p>References:</p>
<ul>
    <li>CERT-EU Security Advisories: <a href="https://cert.europa.eu/publications/security-advisories">https://cert.europa.eu/publications/security-advisories</a></li>
</ul>
<p>CERT-EU provides a list of security advisories that are published on their website.
The advisories are published in a structured manner, with each advisory being contained in a separate JSON file.
The downloader will fetch the list of advisories from the CERT-EU website and download the JSON files for each advisory.
The advisories are structured in a yearly manner, with each year containing a list of advisories published in that year.</p>

| Resource Location | Description |
| --- | --- |
| `YEARLY_PUBLICATIONS_URL` | <p>https://cert.europa.eu/publications/security-advisories/%d</p>HTML page containing a list of all CERT-EU entries published in a single year. <ol>     <li><code>%d</code> Year (example: <code>2020</code>)</li> </ol> |
| `SINGLE_ENTRY_URL` | <p>https://cert.europa.eu/publications/security-advisories/%s/json</p>JSON file containing the details of a single CERT-EU entry. <ol>     <li><code>%s</code> Entry ID (example: <code>2024-042</code>)</li> </ol> |
| `RSS_FEED` | <p>https://cert.europa.eu/publications/security-advisories-rss</p>RSS feed for the latest updated/created CERT-EU entries. |


## CISA KEV

`kev` / `cisaKevDownload`

<details>

<summary>Source Code for <code>CisaKevDownload.performDownload()</code> </summary>

```java
final URL kevURL = getRemoteResourceLocationUrl(ResourceLocationKEV.CISA_KEV);
final File kevFile = new File(super.downloadIntoDirectory, "cisa-kev.json");

super.downloader.fetchResponseBodyFromUrlToFile(kevURL, kevFile);
```

</details>


<p>References:</p>
<ul>
    <li>CISA KEV Homepage: <a href="https://www.cisa.gov/known-exploited-vulnerabilities">https://www.cisa.gov/known-exploited-vulnerabilities</a></li>
</ul>
<p>CISA maintains the authoritative source of vulnerabilities that have been exploited in the wild.
This data is available in a single JSON file that can be downloaded from the CISA website.</p>

| Resource Location | Description |
| --- | --- |
| `CISA_KEV` | <p>https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json</p>URL for the KEV API that is used to fetch all known KEVs known to CISA. |


## EPSS

`epss` / `epssDownload`

<details>

<summary>Source Code for <code>EpssDownload.performDownload()</code> </summary>

```java
final URL epssScoreDownloadUrl = getRemoteResourceLocationUrl(ResourceLocationEpss.EPSS_ALL_SCORES_CSV);
final File downloadIntoFile = getDownloadIntoFile();
super.downloader.fetchResponseBodyFromUrlToFile(epssScoreDownloadUrl, downloadIntoFile);

final ArrayList<String> unpackIssues = new ArrayList<>();
if (!ArchiveUtils.unpackIfPossible(downloadIntoFile, super.getDownloadIntoDirectory(), unpackIssues)) {
    log.error("Issue(s) during unpacking of EPSS scores archive from: {}", downloadIntoFile);
    unpackIssues.forEach(issue -> log.error(" - {}", issue));

    throw new IllegalStateException("Issue(s) during unpacking of EPSS scores archive from: " + downloadIntoFile + "\n - " + String.join("\n - ", unpackIssues));
}

super.propertyFiles.set(super.getDownloadIntoDirectory(), "info", InfoFileAttributes.EPSS_PREFIX.getKey() + "resource-size", String.valueOf(downloadIntoFile.length()));

try {
    FileUtils.delete(downloadIntoFile);
} catch (IOException e) {
    throw new IllegalStateException("Could not delete downloaded EPSS scores archive: " + downloadIntoFile, e);
}
```

</details>


<p>References:</p>
<ul>
<li>EPSS data provider: <a href="https://www.first.org/epss/data_stats">https://www.first.org/epss/data_stats</a></li>
</ul>
<p>The EPSS (Exploit Prediction Scoring System) scores are available as a single CSV file, compressed using GZIP format, which is retrieved from the official EPSS data provider.</p>

| Resource Location | Description |
| --- | --- |
| `EPSS_ALL_SCORES_CSV` | <p>https://epss.cyentia.com/epss_scores-current.csv.gz</p>URL for fetching the current CSV file with the EPSS scores.<br> Data is of format: <pre> #model_version:v2023.03.01,score_date:2024-04-17T00:00:00+0000 cve,epss,percentile CVE-1999-0001,0.00383,0.72764 CVE-1999-0002,0.02080,0.88906 CVE-1999-0003,0.04409,0.92276 CVE-1999-0004,0.00917,0.82643 CVE-1999-0005,0.91963,0.98891 </pre> Default value: <a href="https://epss.cyentia.com/epss_scores-current.csv.gz">https://epss.cyentia.com/epss_scores-current.csv.gz</a> |
| `EPSS_SPECIFIC_DATE_CSV` | <p>https://epss.cyentia.com/epss_scores-%s-%s-%s.csv.gz</p>If you would like historical data for EPSS, you can directly access daily files by entering the target date into the URL in this format: <a href="https://epss.cyentia.com/epss_scores-YYYY-mm-dd.csv.gz">https://epss.cyentia.com/epss_scores-YYYY-mm-dd.csv.gz</a>, where YYYY is the four digit year, mm is the two digit month and dd is a two digit day.<p> For example, if you want to pull February 1st, 2023, you would retrieve <a href="https://epss.cyentia.com/epss_scores-2023-02-01.csv.gz">https://epss.cyentia.com/epss_scores-2023-02-01.csv.gz</a>. <ol>     <li><code>%s</code> - Year (YYYY)</li>     <li><code>%s</code> - Month (mm)</li>     <li><code>%s</code> - Day (dd)</li> </ol> |


## EOL

`eol` / `eolDownload`

<details>

<summary>Source Code for <code>EolDownload.performDownload()</code> </summary>

```java
final URL allProductsUrl = getRemoteResourceLocationUrl(ResourceLocationEOL.EOL_ALL_PRODUCTS);
final JSONArray allProducts = new JSONArray(String.join("", super.downloader.fetchResponseBodyFromUrlAsList(allProductsUrl)));

for (int i = 0; i < allProducts.length(); i++) {
    final String product = allProducts.getString(i);
    final URL productUrl = getRemoteResourceLocationUrl(ResourceLocationEOL.EOL_PRODUCT_VERSIONS, product);

    final JSONArray productVersions = new Retry<>(() -> new JSONArray(String.join("", super.downloader.fetchResponseBodyFromUrlAsList(productUrl))))
            .retryCount(3)
            .withDelay(1000)
            .onFailure(e -> {
                throw new RuntimeException("Failed to fetch EOL product versions.", e);
            })
            .run();

    try {
        FileUtils.write(new File(super.downloadIntoDirectory, product + ".json"), productVersions.toString(), StandardCharsets.UTF_8);
    } catch (IOException e) {
        throw new RuntimeException("Failed to write EOL product info to directory.", e);
    }
}
```

</details>


<p>References:</p>
<ul>
<li>EOL API: <a href="https://endoflife.date/docs/api">https://endoflife.date/docs/api</a></li>
</ul>
<p>This download manages the End of Life (EOL) data for various products using their API.
The API provides a list of product identifiers, and for each product, a detailed version history including end-of-life dates.</p>
<p>The downloader fetches this list of all products and retrieves their individual version information for each product.
Each product's version data is stored locally in JSON files.</p>
<pre><code>.
└── eol
    ├── java.json
    ├── nodejs.json
    ├── python.json
    └── ...
</code></pre>

| Resource Location | Description |
| --- | --- |
| `EOL_PRODUCT_VERSIONS` | <p>https://endoflife.date/api/%s.json</p>URL for the EOL API that is used to fetch the version information for a single product. <ol>     <li><code>%s</code> the product identifier to fetch the version information for (example: <code>java</code>)</li> </ol> |
| `EOL_ALL_PRODUCTS` | <p>https://endoflife.date/api/all.json</p>URL for the EOL API that is used to fetch all product identifiers.<br> Returns an array of strings. |


## _NVD (deprecated)_

`nvd` / `nvdLegacyDownload`

<details>

<summary>Source Code for <code>NvdDownload.performDownload()</code> </summary>

```java
final Map<Integer, String> changedArchiveYears;

if (cachedChangedYearlyArchives == null) {
    changedArchiveYears = fetchChangedArchiveYears();
} else {
    changedArchiveYears = cachedChangedYearlyArchives;
    cachedChangedYearlyArchives = null;
}

LOG.info("Starting downloads for years {}", changedArchiveYears.keySet());

for (Map.Entry<Integer, String> changedYearEntry : changedArchiveYears.entrySet()) {
    super.executor.submit(() ->
            new Retry<>(() -> {
                final Integer year = changedYearEntry.getKey();
                final String lastModifiedDate = changedYearEntry.getValue();

                final URL requestUrl;
                final File archiveFile;
                final File targetFile;
                final String propertyName;

                if (year == META_FILE_YEAR_PLACEHOLDER) {
                    requestUrl = getRemoteResourceLocationUrl(ResourceLocationNvd.CVE_MODIFIED_URL);
                    archiveFile = new File(super.downloadIntoDirectory, "nvd-modified.json.gz");
                    targetFile = new File(super.downloadIntoDirectory, "nvd-modified.json");
                    propertyName = InfoFileAttributes.NVD_PREFIX.getKey() + "last-modified-meta";
                } else {
                    requestUrl = getRemoteResourceLocationUrl(ResourceLocationNvd.CVE_YEAR_BASE_URL, year);
                    archiveFile = new File(super.downloadIntoDirectory, "nvd-" + year + ".json.gz");
                    targetFile = new File(super.downloadIntoDirectory, "nvd-" + year + ".json");
                    propertyName = InfoFileAttributes.NVD_PREFIX.getKey() + "last-modified-" + year;
                }

                if (targetFile.exists()) {
                    targetFile.delete();
                }
                if (archiveFile.exists()) {
                    archiveFile.delete();
                }

                super.downloader.fetchResponseBodyFromUrlToFile(requestUrl, archiveFile);

                ArchiveUtils.unpackIfPossible(archiveFile, super.downloadIntoDirectory, new ArrayList<>());
                archiveFile.delete();

                super.propertyFiles.set(super.downloadIntoDirectory, "info", propertyName, lastModifiedDate);
            }).onException(Exception.class)
                    .retryCount(5)
                    .run()
    );
}

super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for download threads to finish.", e);
}
```

</details>


Uses the deprecated 1.x NVD JSON feeds to download the NVD data.<br>
These data feeds are no longer available and can no longer be used. More information can be found at
<a href="https://nvd.nist.gov/general/news/api-20-announcements">https://nvd.nist.gov/general/news/api-20-announcements</a>.

| Resource Location | Description |
| --- | --- |
| `CVE_MODIFIED_URL` | <p>https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-modified.json.gz</p>The URL to retrieve the recently modified and added CVE entries (last 8 days) using the JSON data feeds.<br> <code>modified</code> feeds are updated every two hours. |
| `CVE_MODIFIED_META_URL` | <p>https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-modified.meta</p>Meta-information on the last modified and added CVE entries, such as the last modified date. |
| `CVE_YEAR_BASE_URL` | <p>https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-%d.json.gz</p>The URL to get a specific yearly CVE list using the JSON data feeds. <ol>     <li><code>%d</code> Year (example: <code>2022</code>)</li> </ol> |
| `CVE_YEAR_META_URL` | <p>https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-%d.meta</p>Meta-information on the specific year files, such as the last modified date.<br> Content is of style: <code>key:value</code> <ol>     <li><code>%d</code> Year (example: <code>2022</code>)</li> </ol> |


## _CPE Dictionary (deprecated)_

`cpe-dict` / `cpeDictionaryDownload`

<details>

<summary>Source Code for <code>CpeDictionaryDownload.performDownload()</code> </summary>

```java
super.executor.submit(() -> {
    try {
        fetchZippedFile(CPE_DICTIONARY_URL, "cpe-dict.xml", "official-cpe-dictionary_v2.2.xml");
    } catch (IOException e) {
        throw new RuntimeException("Unable to download and extract CPE Dictionary from " + CPE_DICTIONARY_URL, e);
    }
});

super.executor.submit(() -> {
    try {
        fetchZippedFile(CPE_MATCH_URL, "cpe-match.json", "nvdcpematch-1.0.json");
    } catch (IOException e) {
        throw new RuntimeException("Unable to download and extract CPE Match from " + CPE_MATCH_URL, e);
    }
});

super.executor.start();
try {
    super.executor.join();
} catch (InterruptedException e) {
    throw new RuntimeException("Failed to wait for download threads to finish.", e);
}
```

</details>


Uses the deprecated 1.x NVD JSON feeds to download the NVD data.<br>
These data feeds are no longer available and can no longer be used. More information can be found at
<a href="https://nvd.nist.gov/general/news/api-20-announcements">https://nvd.nist.gov/general/news/api-20-announcements</a>.

| Resource Location | Description |
| --- | --- |
| `CPE_DICTIONARY_URL` | <p>https://nvd.nist.gov/feeds/xml/cpe/dictionary/official-cpe-dictionary_v2.2.xml.zip</p>The official CPE dictionary: <a href="https://nvd.nist.gov/products/cpe">https://nvd.nist.gov/products/cpe</a> |
| `CPE_MATCH_URL` | <p>https://nvd.nist.gov/feeds/json/cpematch/1.0/nvdcpematch-1.0.json.zip</p>The CPE match data feed contains additional information on the specific CPE metadata: <a href="https://nvd.nist.gov/vuln/data-feeds#cpeMatch">https://nvd.nist.gov/vuln/data-feeds#cpeMatch</a> |


## _MSRC Manual CSV (deprecated)_

`msrc-csv` / `msrcCsvDownload`

<details>

<summary>Source Code for <code>MsrcManualCsvDownload.performDownload()</code> </summary>

```java
final File[] files = super.downloadIntoDirectory.listFiles();
final int fileCount = files == null ? 0 : (int) Arrays.stream(files).filter(file -> file.getName().endsWith(".csv")).count();

if (fileCount == 0) {
    LOG.error("No files found in {}", super.downloadIntoDirectory);
    LOG.error("No files found for [msrc-csv]. Visit https://msrc.microsoft.com/update-guide/vulnerability to manually download the CSV files into {}", super.downloadIntoDirectory);
    setCsvCount(0);
} else {
    LOG.info("Found {} files in {}. Assuming download was successful.", fileCount, super.downloadIntoDirectory);
    setCsvCount(fileCount);
}
```

</details>



